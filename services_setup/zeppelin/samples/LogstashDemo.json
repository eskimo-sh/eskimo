{
  "paragraphs": [
    {
      "text": "%md \n\n### 1. Cleanup : Delete raw data elasticsearch indices\n\nJust a cleaning step where the elasticsearch indices corresponding to raw data are deleted",
      "user": "anonymous",
      "dateUpdated": "2020-06-13T11:41:01+0000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "runOnSelectionChange": true,
        "title": false,
        "checkEmpty": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>1. Cleanup : Delete raw data elasticsearch indices</h3>\n<p>Just a cleaning step where the elasticsearch indices corresponding to raw data are deleted</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1574265140711_2075889415",
      "id": "paragraph_1573144613610_1164885594",
      "dateCreated": "2019-11-20T15:52:20+0000",
      "dateStarted": "2019-11-20T15:53:23+0000",
      "dateFinished": "2019-11-20T15:53:25+0000",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:453"
    },
    {
      "text": "%sh\n\n# Delete all berka indices\ncurl -XDELETE 'http://elasticsearch.eskimo.svc.cluster.eskimo:9200/berka-*'\n",
      "user": "anonymous",
      "dateUpdated": "2020-06-13T15:43:57+0000",
      "config": {
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false,
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/sh",
        "fontSize": 9,
        "editorHide": false,
        "runOnSelectionChange": true,
        "title": false,
        "checkEmpty": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1574265140719_-1212682067",
      "id": "paragraph_1573144770167_1974251292",
      "dateCreated": "2019-11-20T15:52:20+0000",
      "dateStarted": "2020-06-13T13:11:59+0000",
      "dateFinished": "2020-06-13T13:12:00+0000",
      "status": "FINISHED",
      "$$hashKey": "object:454"
    },
    {
      "text": "%md \n\n### 2. Define Index template for berka data",
      "user": "anonymous",
      "dateUpdated": "2020-06-13T11:41:17+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>2. Define Index template for berka data</h3>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1592048248549_875848488",
      "id": "paragraph_1592048248549_875848488",
      "dateCreated": "2020-06-13T11:37:28+0000",
      "dateStarted": "2020-06-13T11:41:17+0000",
      "dateFinished": "2020-06-13T11:41:19+0000",
      "status": "FINISHED",
      "$$hashKey": "object:455"
    },
    {
      "text": "%sh\n\ncurl -XPUT -H 'Content-Type: application/json' http://elasticsearch.eskimo.svc.cluster.eskimo:9200/_template/berka-template -d '\n{\n  \"index_patterns\": [\"berka*\"],\n  \"mappings\": {\n    \"dynamic_templates\": [\n      {\n        \"amount_as_float\": {\n          \"match_mapping_type\": \"string\",\n          \"match\":   \"amount\",\n          \"mapping\": {\n            \"type\": \"float\"\n          }\n        }\n      },\n      {\n        \"balance_as_float\": {\n          \"match_mapping_type\": \"string\",\n          \"match\":   \"balance\",\n          \"mapping\": {\n            \"type\": \"float\"\n          }\n        }\n      }\n    ]\n  }\n}'",
      "user": "anonymous",
      "dateUpdated": "2020-06-13T13:11:36+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1592048236546_788859320",
      "id": "paragraph_1592048236546_788859320",
      "dateCreated": "2020-06-13T11:37:16+0000",
      "dateStarted": "2020-06-13T13:11:36+0000",
      "dateFinished": "2020-06-13T13:11:36+0000",
      "status": "FINISHED",
      "$$hashKey": "object:456"
    },
    {
      "text": "%md \n### 3. Download Berka dataset from niceideas.ch\n\nHere we download the individual files from the Berka Dataset from niceideas.ch and put them in /var/lib/elasticsearch/logstash/data/test_berka/\n/var/lib/elasticsearch/logstash/data/ is a folder shared between the logstash docker containre and the zeppelin docker container.\nIn a multi-node cluster deployment, /var/lib/elasticsearch/logstash/data/test_berka/ is shared between all nodes using gluster.\n",
      "user": "anonymous",
      "dateUpdated": "2020-06-13T11:41:33+0000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "runOnSelectionChange": true,
        "title": false,
        "checkEmpty": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>3. Download Berka dataset from niceideas.ch</h3>\n<p>Here we download the individual files from the Berka Dataset from niceideas.ch and put them in /var/lib/elasticsearch/logstash/data/test_berka/<br />\n/var/lib/elasticsearch/logstash/data/ is a folder shared between the logstash docker containre and the zeppelin docker container.<br />\nIn a multi-node cluster deployment, /var/lib/elasticsearch/logstash/data/test_berka/ is shared between all nodes using gluster.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1574265140719_1663799918",
      "id": "paragraph_1573145010348_1692553399",
      "dateCreated": "2019-11-20T15:52:20+0000",
      "dateStarted": "2020-06-13T11:41:33+0000",
      "dateFinished": "2020-06-13T11:41:33+0000",
      "status": "FINISHED",
      "$$hashKey": "object:457"
    },
    {
      "text": "%sh\n\n# Have to use /var/lib/elasticsearch/logstash/data/ since it's shared between logstash and zeppelin containers\nmkdir -p /var/lib/elasticsearch/logstash/data/test_berka/\n\n# Downloading all berka datasets\ndownload_dataset() {\n    if [[ ! -f \"/var/lib/elasticsearch/logstash/data/test_berka/$1.asc\" ]]; then\n        echo \" - downloading berka $1 dataset\"\n        wget http://niceideas.ch/mes/berka/$1.asc > /tmp/download_data 2>&1    \n        if [[ $? != 0 ]]; then\n            echo \"-> Failed to download berka/$1.asc data from niceideas.ch \"\n            cat /tmp/download_data\n        fi\n        mv $1.asc /var/lib/elasticsearch/logstash/data/test_berka/$1.asc\n    fi\n}\n\n# Now call the 'download_dataset' function defined above for all datasets\ndownload_dataset account\ndownload_dataset card\ndownload_dataset client\ndownload_dataset disp\ndownload_dataset district\ndownload_dataset loan\ndownload_dataset order\ndownload_dataset trans\n",
      "user": "anonymous",
      "dateUpdated": "2020-06-12T19:55:56+0000",
      "config": {
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false,
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/sh",
        "fontSize": 9,
        "runOnSelectionChange": true,
        "title": false,
        "checkEmpty": true,
        "results": {},
        "enabled": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1574265140721_-1273453641",
      "id": "paragraph_1573144840643_524948953",
      "dateCreated": "2019-11-20T15:52:20+0000",
      "dateStarted": "2020-06-12T19:55:56+0000",
      "dateFinished": "2020-06-12T19:56:15+0000",
      "status": "FINISHED",
      "$$hashKey": "object:458"
    },
    {
      "text": "%md\n\n### 4. Ingest Raw Berka Data in ElasticSearch\n\nIn this paragraph, we use logstash-cli to ingest the individual Berka datasets into ElasticSearch, in different indices.\nlogstash-cli is a client using HTTP to reach a command server running on the logstash container.\n\n*Notes*:\n\n* Warning : STDIN piping is done with logstash-cli using the specific -std_in argument.\n* **If the logstash service is not installed on every node of the eskimo cluster, you have to change the \"localhost\" target host in the code below**",
      "user": "anonymous",
      "dateUpdated": "2020-06-13T11:41:49+0000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "runOnSelectionChange": true,
        "title": false,
        "checkEmpty": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>4. Ingest Raw Berka Data in ElasticSearch</h3>\n<p>In this paragraph, we use logstash-cli to ingest the individual Berka datasets into ElasticSearch, in different indices.<br />\nlogstash-cli is a client using HTTP to reach a command server running on the logstash container.</p>\n<p><em>Notes</em>:</p>\n<ul>\n<li>Warning : STDIN piping is done with logstash-cli using the specific -std_in argument.</li>\n<li><strong>If the logstash service is not installed on every node of the eskimo cluster, you have to change the &ldquo;localhost&rdquo; target host in the code below</strong></li>\n</ul>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1574265140721_2124158508",
      "id": "paragraph_1573145120689_-1755825542",
      "dateCreated": "2019-11-20T15:52:20+0000",
      "dateStarted": "2020-06-13T11:41:49+0000",
      "dateFinished": "2020-06-13T11:41:49+0000",
      "status": "FINISHED",
      "$$hashKey": "object:459"
    },
    {
      "text": "%sh\n\n. /etc/eskimo_topology.sh\n\n# logstash ingestion configuration\ningest_dataset() {\n\n    echo -e \"\ninput {\n    stdin { }\n}\nfilter {\n  csv {\n    separator => \\\";\\\"\n    columns => [$3] \n  }\n  if ([col1] == \\\"$2\\\") {\n    drop { }\n  }  \n  mutate {\n    rename => { \\\"@timestamp\\\" => \\\"timestamp\\\" }\n  }\n  date {\n    match => [ \\\"$4\\\", \\\"$5\\\" ]\n    target => \\\"$6\\\"\n  }\n}\noutput {\n   if \\\"_dateparsefailure\\\" not in [tags] {\n      elasticsearch {\n        hosts => \\\"http://elasticsearch.eskimo.svc.cluster.eskimo:9200\\\"\n        index => \\\"berka-$1\\\"\n      }\n   }\n}\n\" > /var/lib/elasticsearch/logstash/data/test_berka/import_berka_$1.conf\n\n\n    # logstash call (using logstash container on same host)\n    echo \" - Executing logstash on $1 (this takes time)\"\n    rm -f /tmp/logstash_raw_berka_$1.log\n    logstash-cli -std_in /var/lib/elasticsearch/logstash/data/test_berka/$1.asc -f /var/lib/elasticsearch/logstash/data/test_berka/import_berka_$1.conf > /tmp/logstash_raw_berka_$1.log \n    if [[ $? != 0 ]]; then\n        echo \"-> Failed !\"\n        cat /tmp/logstash_raw_berka_$1.log \n    fi\n    \n}\n\ningest_dataset account '\\\"account_id\\\"' '\"account_id\",\"district_id\",\"frequency\",\"date\"' date 'YYMMdd' creation_date\ningest_dataset card '\\\"card_id\\\"' '\"card_id\",\"disp_id\",\"type\",\"issued\"' issued 'YYMMdd 00:00:00' creation_date\ningest_dataset client '\\\"client_id\\\"' '\"client_id\",\"birth_number\",\"district_id\"' none 'YYMMdd' creation_date\ningest_dataset disp '\\\"disp_id\\\"' '\"disp_id\",\"client_id\",\"account_id\",\"type\"' none 'YYMMdd' creation_date\ningest_dataset district 'A1' '\"district_id\",\"district_name\",\"region_name\",\"num_inhabitants\",\"num_munipalities_gt499\",\"num_munipalities_500to1999\",\"num_munipalities_2000to9999\",\"num_munipalities_gt10000\",\"num_cities\",\"ratio_urban\",\"average_salary\",\"unemp_rate95\",\"unemp_rate96\",\"num_entrep_per1000\",\"num_crimes95\",\"num_crimes96\"'  none 'YYMMdd' creation_date\ningest_dataset loan '\\\"load_id\\\"' '\"loan_id\",\"account_id\",\"date\",\"amount\",\"duration\",\"payments\",\"status\"' date 'YYMMdd' @timestamp\ningest_dataset order '\\\"order_id\\\"' '\"order_id\",\"account_id\",\"bank_to\",\"account_to\",\"amount\",\"k_symbol\"' none 'YYMMdd' creation_date\ningest_dataset trans '\\\"trans_id\\\"' '\"trans_id\",\"account_id\",\"date\",\"type\",\"operation\",\"amount\",\"balance\",\"k_symbol\",\"bank\",\"account\"' date 'YYMMdd' @timestamp\n\n\n",
      "user": "anonymous",
      "dateUpdated": "2020-06-13T13:12:22+0000",
      "config": {
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false,
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/sh",
        "fontSize": 9,
        "runOnSelectionChange": true,
        "title": false,
        "checkEmpty": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1574265140722_-108928902",
      "id": "paragraph_1573145219945_2112967654",
      "dateCreated": "2019-11-20T15:52:20+0000",
      "dateStarted": "2020-06-13T13:12:22+0000",
      "dateFinished": "2020-06-13T13:30:44+0000",
      "status": "FINISHED",
      "$$hashKey": "object:460"
    },
    {
      "text": "%md\n\n### Data is now on ElasticSearch. Go to kibana and visualize it :-)\n",
      "user": "anonymous",
      "dateUpdated": "2019-11-20T15:52:20+0000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "editOnDblClick": true,
          "completionSupport": false,
          "language": "markdown"
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "runOnSelectionChange": true,
        "title": false,
        "checkEmpty": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>Data is now on ElasticSearch. Go to kibana and visualize it :-)</h3>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1574265140722_-1556743178",
      "id": "paragraph_1573145385601_1262117494",
      "dateCreated": "2019-11-20T15:52:20+0000",
      "status": "READY",
      "$$hashKey": "object:461"
    }
  ],
  "name": "Logstash Demo",
  "id": "2EURNXTM1",
  "defaultInterpreterGroup": "spark",
  "version": "0.9.0-SNAPSHOT",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {},
  "path": "/Logstash Demo",
  "checkpoint": {
    "message": "ss"
  }
}