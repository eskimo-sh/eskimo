{
  "paragraphs": [
    {
      "text": "%md\n\n\n### Spark and ElasticSearch Integration\n\n\nThis notebook presents a sample program illustrating Spark (Batch) and ElasticSearch Integration.\n\nThis sample program uses the Berka DataSet loaded in ElasticSearch by the \"Logstash Demo\" Notebook and flattens the transaction dataset by denormalizing the related data into the transaction dataset.\n\n| **warning: You need to have executed all the Paragraphs from the \"Logstash Demo\" Notebook before you can run this notebook !** |\n| --- |\n",
      "user": "anonymous",
      "dateUpdated": "2019-12-30 08:36:32.304",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "runOnSelectionChange": true,
        "title": false,
        "checkEmpty": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eSpark and ElasticSearch Integration\u003c/h3\u003e\n\u003cp\u003eThis notebook presents a sample program illustrating Spark (Batch) and ElasticSearch Integration.\u003c/p\u003e\n\u003cp\u003eThis sample program uses the Berka DataSet loaded in ElasticSearch by the \u0026ldquo;Logstash Demo\u0026rdquo; Notebook and flattens the transaction dataset by denormalizing the related data into the transaction dataset.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\u003cth\u003e\u003cstrong\u003ewarning: You need to have executed all the Paragraphs from the \u0026ldquo;Logstash Demo\u0026rdquo; Notebook before you can run this notebook !\u003c/strong\u003e\u003c/th\u003e\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1575966915510_-1727530779",
      "id": "paragraph_1575966915510_-1727530779",
      "dateCreated": "2019-12-10 08:35:15.510",
      "dateStarted": "2019-12-30 08:36:32.309",
      "dateFinished": "2019-12-30 08:36:32.360",
      "status": "FINISHED"
    },
    {
      "text": "%pyspark\n\n\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SQLContext, SparkSession\nimport pyspark.sql.functions as F\n\n## Spark SQL Session \n#ss \u003d SparkSession.builder \\\n#        .config(conf\u003dconf) \\\n#        .getOrCreate()\n\n\n# Query configuration only (cannot pass any ES conf here :-( )\nes_query_conf\u003d { \n    \"pushdown\": True\n}\n\n# Zeppelin calls the spark session \"spark\n# ss is perhaps politically unsure but is really much more standard\nss \u003d spark\n\n# Every time there is a shuffle, Spark needs to decide how many partitions will \n# the shuffle RDD have. \n# 2 times the amount of CPUS in the cluster is a good value (default is 200) \nss.conf.set(\"spark.sql.shuffle.partitions\", \"12\")\n\n# Read transactions dataset\ntrans_df \u003d ss.read \\\n        .format(\"org.elasticsearch.spark.sql\") \\\n        .options(conf\u003des_query_conf) \\\n        .load(\"berka-trans\") \\\n        .alias(\"trans_df\")\n\n\n# 1. Joining on Account\n# ---------------------------------------------------------\n\n# Read account dataset\naccount_df \u003d ss.read \\\n        .format(\"org.elasticsearch.spark.sql\") \\\n        .options(conf\u003des_query_conf) \\\n        .load(\"berka-account\") \\\n        .alias(\"account_df\")\n\n# Join on account_id\naccount_joint_df \u003d trans_df \\\n            .join( \\\n                  account_df, \\\n                  (F.lower(trans_df.account_id) \u003d\u003d F.lower(account_df.account_id)), \\\n                  \"left_outer\" \\\n                 ) \\\n            .select( \\\n                    \u0027trans_id\u0027, \u0027trans_df.account_id\u0027, \\\n                    F.col(\"trans_df.@timestamp\").alias(\"@timestamp\"), \\\n                    F.col(\"trans_df.@timestamp\").alias(\"value_date\"), \u0027type\u0027, \u0027operation\u0027, \u0027amount\u0027, \u0027balance\u0027, \u0027k_symbol\u0027, \\\n                    F.col(\"bank\").alias(\"beneficiary_bank\"), F.col(\"account\").alias(\"beneficiary_account\"), \\\n                    F.col(\"district_id\").alias(\"account_district_id\"), \\\n                    F.col(\"frequency\").alias(\"account_frequency\")\n                   )\n\n\n# 2. Joining on Disp\n# ---------------------------------------------------------\n\n# Read disp dataset\ndisp_df \u003d ss.read \\\n        .format(\"org.elasticsearch.spark.sql\") \\\n        .options(conf\u003des_query_conf) \\\n        .load(\"berka-disp\") \\\n        .alias(\"disp_df\") \\\n        .filter(F.col (\"type\") \u003d\u003d \"OWNER\")\n\ndisp_joint_df \u003d account_joint_df \\\n            .join( \\\n                  disp_df, \\\n                  (F.lower(account_joint_df.account_id) \u003d\u003d F.lower(disp_df.account_id)), \\\n                  \"left_outer\" \\\n                 ) \\\n            .select( \\\n                    \u0027trans_id\u0027, F.col (\u0027trans_df.account_id\u0027).alias (\u0027account_id\u0027), \"@timestamp\", \\\n                    \u0027value_date\u0027, F.col(\"trans_df.type\").alias(\"transaction_type\"), \u0027operation\u0027, \u0027amount\u0027, \u0027balance\u0027, \u0027k_symbol\u0027, \u0027beneficiary_bank\u0027, \u0027beneficiary_account\u0027, \\\n                    \u0027account_district_id\u0027, \u0027account_frequency\u0027, \\\n                    \u0027disp_id\u0027, \u0027client_id\u0027, F.col(\"disp_df.type\").alias(\"disp_access_type\")\n                   )\n\n\n# 3. Joining on Client\n# ---------------------------------------------------------\n\n# Read client dataset\nclient_df \u003d ss.read \\\n        .format(\"org.elasticsearch.spark.sql\") \\\n        .options(conf\u003des_query_conf) \\\n        .load(\"berka-client\") \\\n        .alias(\"client_df\")\n\nclient_joint_df \u003d disp_joint_df \\\n            .join( \\\n                  client_df, \\\n                  (F.lower(disp_joint_df.client_id) \u003d\u003d F.lower(client_df.client_id)), \\\n                  \"left_outer\" \\\n                 ) \\\n            .select( \\\n                    \u0027trans_id\u0027, \u0027account_id\u0027, \u0027@timestamp\u0027, \u0027value_date\u0027, \u0027transaction_type\u0027, \u0027operation\u0027, \u0027amount\u0027, \\\n                    \u0027balance\u0027, \u0027k_symbol\u0027, \u0027beneficiary_bank\u0027, \u0027beneficiary_account\u0027, \\\n                    \u0027account_district_id\u0027, \u0027disp_id\u0027, \u0027account_frequency\u0027, \n                    F.col (\u0027client_df.client_id\u0027).alias (\u0027client_id\u0027), \n                    \u0027disp_access_type\u0027,\n                    F.col (\"birth_number\").alias(\"client_birth_number\"),\n                    F.col (\"district_id\").alias(\"client_district_id\")\n                   )\n\n\n\n\n# 4. Joining on District\n# ---------------------------------------------------------\n\n# Read district dataset\ndistrict_df \u003d ss.read \\\n        .format(\"org.elasticsearch.spark.sql\") \\\n        .options(conf\u003des_query_conf) \\\n        .load(\"berka-district\") \\\n        .alias(\"district_df\")\n\ndistrict_joint_df \u003d client_joint_df \\\n            .join( \\\n                  district_df, \\\n                  (F.lower(client_joint_df.client_district_id) \u003d\u003d F.lower(district_df.district_id)), \\\n                  \"left_outer\" \\\n                 ) \\\n            .select( \\\n                    \u0027trans_id\u0027, \u0027account_id\u0027, \\\n                    F.date_format(F.col(\u0027@timestamp\u0027), \"yyyy-MM-dd\u0027T\u0027HH:mm:ssZZ\").alias(\u0027@timestamp\u0027), \\\n                    F.date_format(F.col(\u0027value_date\u0027), \"yyyy-MM-dd\u0027T\u0027HH:mm:ssZZ\").alias(\u0027value_date\u0027), \\\n                    \u0027transaction_type\u0027, \u0027operation\u0027, \\\n                    \u0027amount\u0027, \u0027balance\u0027, \u0027k_symbol\u0027, \u0027beneficiary_bank\u0027, \u0027beneficiary_account\u0027, \\\n                    \u0027account_district_id\u0027, \u0027disp_id\u0027, \u0027account_frequency\u0027, \u0027client_id\u0027, \n                    \u0027disp_access_type\u0027, \u0027client_birth_number\u0027, \u0027client_district_id\u0027,\n                    \u0027district_name\u0027, \u0027region_name\u0027\n                   )\n\n\n# 5. Save transactions\n# ---------------------------------------------------------\n\n\n# # (3) Collect result to the driver\n# join_transactions_list \u003d district_joint_df.collect()\n# \n# print (\"Printing 10 first results\")\n# for x in join_transactions_list[0:10]:\n#     print x\n# \n# # Print count \n# print (\"Computed %s positions (from collected list)\") % len (join_transactions_list)\n\n\n#district_joint_df.write \\\n#        .format(\"org.elasticsearch.spark.sql\") \\\n#        .option(\"es.resource\", \"berka-trans-join/doc\") \\\n#        .mode(saveMode\u003d\"Overwrite\") \\\n#        .save()\n\n\n# 6. Filter in payments only\n# ---------------------------------------------------------\n\n\npayment_df \u003d district_joint_df.filter( (F.col(\"transaction_type\") \u003d\u003d \"VYDAJ\") \u0026 (F.col(\"operation\") !\u003d \"VYBER\") )\n\n\npayment_df.write \\\n        .format(\"org.elasticsearch.spark.sql\") \\\n        .option(\"es.resource\", \"berka-payments/doc\") \\\n        .mode(saveMode\u003d\"Overwrite\") \\\n        .save()\n\n\n",
      "user": "anonymous",
      "dateUpdated": "2019-12-29 09:06:35.923",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "runOnSelectionChange": true,
        "title": false,
        "checkEmpty": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1575966927372_-290792050",
      "id": "paragraph_1575966927372_-290792050",
      "dateCreated": "2019-12-10 08:35:27.372",
      "dateStarted": "2019-12-29 09:06:35.943",
      "dateFinished": "2019-12-29 09:09:14.366",
      "status": "FINISHED"
    },
    {
      "text": "%md\n\n**You can now find  the result joined index in \"berka-payments\" on ElasticSearch**\n\n",
      "user": "anonymous",
      "dateUpdated": "2019-12-10 10:01:25.423",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1575967108779_-1236820496",
      "id": "paragraph_1575967108779_-1236820496",
      "dateCreated": "2019-12-10 08:38:28.779",
      "status": "READY"
    }
  ],
  "name": "Spark Integration ES",
  "id": "2EUG9JP4N",
  "defaultInterpreterGroup": "spark",
  "version": "0.9.0-SNAPSHOT",
  "permissions": {},
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {},
  "path": "/Spark Integration ES"
}