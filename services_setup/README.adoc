////
This file is part of the eskimo project referenced at www.eskimo.sh. The licensing information below apply just as
well to this individual file than to the Eskimo Project as a whole.

Copyright 2019 www.eskimo.sh - All rights reserved.
Author : http://www.eskimo.sh

Eskimo is available under a dual licensing model : commercial and GNU AGPL.
If you did not acquire a commercial licence for Eskimo, you can still use it and consider it free software under the
terms of the GNU Affero Public License. You can redistribute it and/or modify it under the terms of the GNU Affero
Public License  as published by the Free Software Foundation, either version 3 of the License, or (at your option)
any later version.
Compliance to each and every aspect of the GNU Affero Public License is mandatory for users who did no acquire a
commercial license.

Eskimo is distributed as a free software under GNU AGPL in the hope that it will be useful, but WITHOUT ANY
WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
Affero Public License for more details.

You should have received a copy of the GNU Affero Public License along with Eskimo. If not,
see <https://www.gnu.org/licenses/> or write to the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
Boston, MA, 02110-1301 USA.

You can be released from the requirements of the license by purchasing a commercial license. Buying such a
commercial license is mandatory as soon as :
- you develop activities involving Eskimo without disclosing the source code of your own product, software,
  platform, use cases or scripts.
- you deploy eskimo as part of a commercial product, platform or software.
For more information, please contact eskimo.sh at https://www.eskimo.sh

The above copyright notice and this licensing notice shall be included in all copies or substantial portions of the
Software.
////


:sectnums:
:authors: www.eskimo.sh / 2019
:copyright: www.eskimo.sh / 2019

== Eskimo Introduction

Eskimo is a Big Data Management Web Console to _build_, _manage_ and _operate_
*Big Data 2.0* clusters using *Docker* and *Mesos*.

Reach http://www.eskimo.sh for more information on Eskimo or look at the documentation in the folder `doc`.

=== Eskimo Service Development Framework

The *Service Development framework* is actually composed by two distinct parts:

1. The *Docker Images Development Framework* which is used to build the docker images deployed on the eskimo cluster
nodes
2. The *Services Installation Framework* which is used to install these images as services on the eskimo cluster nodes.

This document presents "2. The *Services Installation Framework*"




// marker for inclusion : line 60
[[services_installation_framework]]
== Services Installation Framework

The Services Installation Framework provides tools and standards to install the packaged docker images containing the
target software component on the eskimo cluster nodes.

Eskimo is leveraging on _docker_ to run the the services on the cluster nodes and either _SystemD_ or _Marathon_ to
operate them. Marathon is used specifically to provide HA - High Availability - and resilience to UI applications and
other single instances services

* An eskimo package has to be a docker image
* An eskimo package has to provide
** either a systemd unit configuration file to enable eskimo to operate the component on the cluster.
** or a marathon JSON configuration file to delegate this marathon

The Eskimo Nodes Configuration process takes care of installing the services defined in the _services.json_
configuration file and and copies them over to the nodes where they are intended to run. After the proper installation,
eskimo relies either on plain old `systemctl` commands to operate (start / stop / restart / query / etc.) the services
or on _marathon_

=== Principle

The principle is pretty straightforward:

* Whenever a service `serviceX` is configured on a node, eskimo makes an archive of the folder
`services_setup/serviceX`, copies that archive over to the node and extracts it in a subfolder of `/tmp`.
* Then eskimo calls the script `setup.sh` from within that folder. The script `setup.sh` can do whatever it wants but has
to respect the following constraint:
* After that `setup.sh` script is properly executed, the service should be
** either installed on the node along with a systemd system unit file with name `serviceX.service` which is used for
commands such as `systemctl start serviceX` to be able to control service _serviceX_,
** or properly registered in marathon and operated by marathon with ID _serviceX_.

Aside from the above, nothing is enforced and service developers can implement services the way they want.

==== Gluster share mounts

Services running through System mount gluster shares directly on the host machine through scripts operated with
`ExecStartPre` directives of their respective SystemD unit files. Then the gluster shares they require is mounted in
their runtime docker containers.

The same approach is impossible for marathon services since marathon cannot hook any command on the host machine in
prior of the docker container start. For this reason, services operated by marathon mount gluster shares directly in
their runtime docker containers.

==== OS System Users creation

OS system users required to execute marathon services are required on every node of the cluster. For this reason,
the linux system users to be created on every node are not created in the individual services `setup.sh` scripts. They
are created in the eskimo base system installation script `install-eskimo-base-system.sh` in the function
`create_common_system_users`.

=== Standards and conventions over requirements

There are no requirements when setting up a service on a node aside from the constraints mentioned above.
Services developers can set up services on nodes the way then want and no specific requirement is enforced by eskimo.

However, adhering to some conventions eases a lot the maintenance and evolution of these services. +
These standard conventions are as follows (illustrated for a service called `serviceX`).

* Services should put their persistent data (to be persisted between two docker container restart) in `/var/lib/serviceX`
* Services should put their log files in `/var/log/serviceX`.
* If the service required a file to track its PID, that file should be stored under `/var/run/serviceX`
* Whenever a service `serviceX` requires a subfolder of `/var/log/serviceX` to be shared among cluster nodes, a script
`setupServiceXGlusterSares.sh` should be defined that calls the common helper script (define at eskimo base system
installation on every node) `/usr/local/sbin/gluster_mount.sh` in the following way, for instance to define
 the _flink data_ share : `/usr/local/sbin/gluster_mount.sh flink_data /var/lib/flink/data flink`

At the end of the day, it's really plain old Unix standards. The only challenge comes from the use of docker which
requires to play with docker mounts a little. +
Just look at eskimo pre-packaged services to see examples.

=== Typical setup.sh process

==== Operations performed

The setup process implemented as a standard in the `setup.sh` script has three different stages:

. The container instantiation from the pre-packaged image performed from outside the container
. The software component setup performed from inside the container
- The registration of the service to _SystemD_ or _marathon_
. The software component configuration applied at runtime, i.e. at the time the container starts, re-applied everytime.

The fourth phase is most of the time required to apply configurations depending on environment dynamically at startup
time and not statically at setup time. +
The goal is to address situations where, for instance,  master services are moved to another node. In this case,
applying the master setup configuration at service startup time instead of statically enables to simply restart a slave
service whenever the master node is moved to another node instead of requiring to entirely re-configure them.

The install and setup process thus typically looks this way:

1. From outside the container:
* Perform required configurations on host OS (create `/var/lib` subfolder, required system user, etc.)
* Run docker container that will be used to create the set up image
* Call in container setup script

2. From inside the container:
* Create the in container required folders and system user, etc.
* Adapt configuration files to eskimo context (static configuration only !)

3. At service startup time:
* Adapt configuration to topology (See <<topology>> below)
* Start service

And that's it.

Again, the most essential configuration, the adaptation to the cluster _topology_ is not done statically at container
setup time but dynamically at service startup time.

==== Standard and conventions

While nothing is really enforced as a requirement by eskimo (aside of systemd and the name of the `setup.sh` script,
there are some standards that should be followed (illustrated for a service named `serviceX`:

* The in container setup script is usually called `inContainerSetupServiceX.sh`
* The script taking care of the dynamic configuration and the starting of the service - the one actually called by
systemd upon service startup - is usually called `inContainerStartServiceX.sh`
* The systemd system configuration file is usually limited to stopping and starting the docker container


==== Look for examples and get inspired

Look at examples and the way the packages provided with eskimo are setup and get inspired for developing your own
packages.


=== Eskimo services configuration

Creating the service setup folder and writing the `setup.sh` script is unfortunately not sufficient for eskimo to be
able to operate the service. +
A few additional steps are required, most importantly, defining the new service in the configuration file
`services.json`.

==== Configuration file `services.json`

In order for a service to be understood and operable by eskimo, it needs to be declared in the *services configuration
file*  `services.json`.

A service declaration in `services.json` for instance for `serviceX` would be defined as follows:

.ServiceX declaration in `services.json`
----
"serviceX" : {

  "config": {

    ## [mandatory] giving the column nbr in status table
    "order": [0-X],

    ## [mandatory] whether or not it has to be instaled on every node
    "mandatory": [true,false],

    ## [unique] whether the service is a unique service (singpe instance) or multiple
    "unique": [true,false],

    ## [unique] whether the service is managed through marathon (true) or SystemD (false)
    "marathon": [true,false],

    ## [optional] name of the group to associate it in the status table
    "group" : "{group name}",

    ## [mandatory] name of the service. miust be consistent with service under
    ## 'service_setup'
    "name" : "{service name},

    ## [mandatory] where to place the service in 'Service Selection Window'
    "selectionLayout" : {
      "row" : [1-X],
      "col" : [1-X]
    },

    ## memory to allocate to the service
    ## (mesos and neglectable means the service is excluded from the hardware
    ## memory split computation. Meos services are put in the 'mesos-agent' memory
    ## share)
    "memory": "[mesos|neglectable|small|medium|large|verylarge]",

    ## [mandatory] The logo to use whenever displaying the service in the UI is
    ##     required
    ## Use "images/{logo_file_name}" for resources packaged within eskimo web app
    ## Use "static_images/{logo_file_name}" for resources put in the eskimo
    ##    distribution folder "static_images"
    ## (static_images is configurable in eskimo.properties with property
    ##    eskimo.externalLogoAndIconFolder)
    "logo" : "[images|static_images]/{logo_file_name}"

    ## [mandatory] The icon to use ine the menu for the service
    ## Use "images/{icon_file_name}" for resources packaged within eskimo web app
    ## Use "static_images/{icon_file_name}" for resources put in the eskimo
    ##    distribution folder "static_images"
    ## (static_images is configurable in eskimo.properties with property
    ##    eskimo.externalLogoAndIconFolder)
    "icon" : "[images|static_images]/{icon_file_name}"
  },

  ## [optional] configuration of the serice web console (if anym)
  "ui": {

    ## [optional] (A) either URL template should be configured ...
    "urlTemplate": "http://{NODE_ADDRESS}:{PORT}/",

    ## [optional] (B) .... or proxy configuration in case the service has
    ## to be proxied by eskimo
    "proxyTargetPort" : {target port},

    ## [mandatory] the time  to wait for the web console to initialize before
    ## making it available
    "waitTime": {1000 - X},

    ## [mandatory] the name of the menu entry
    "title" : "{menu name}",

    ## [optional] Whether standard rewrite rules need to be applied to this service
    ## (Standard rewrite rules are documented hereunder)
    ## (default is true)
    "applyStandardProxyReplacements": [true|false],

    ## [optional] List of custom rewrite rules for proxying of web consoles
    "proxyReplacements" : [

      ## first rewrite rule. As many as required can be declared
      {

        ## [mandatory] Type of rwrite rule. At the moment only PLAIN is supported
        ## for full text search and replace.
        ## In the future REGEXP type shall be implemented
        "type" : "[PLAIN]",

        ## [optional] a text searched in the URL. this replacement is applied only
        ## if the text is found in the URL
        "urlPattern" : "{url_pattern}", ## e.g. controllers.js

        ## [mandatory] source text to be replaced
        "source" : "{source_URL}", ## e.g. "/API"

        ## [mandatory] replacement text
        "target" : "{proxied_URL}" ## e.g. "/eskimo/kibana/API"
      }
  },

  ## [optional] array of dependencies that need to be available and configured
  "dependencies": [
    {

      ## [mandatory] THIS IS THE MOST ESSENTIAL CONFIG :
      ## THE WAY THE MASTER IS IDENTIFIED FOR A SLSAVE SERVICE
      "masterElectionStrategy": "[NONE|FIRST_NODE|SAME_NODE_OR_RANDOM|RANDOM|RANDOM_NODE_AFTER|SAME_NODE]"

      ## the service relating to this dependency
      "masterService": "{master service name}",

      ## The number of master expected
      "numberOfMasters": [1-x],

      ## whether that dependency is mandatory or not
      "mandatory": [true,false],
    }
  ]

  ## [optional] array of configuration properties that should be editable using the Eskim UI
  ## These configuration properties are injected
  "editableConfigurations": [
    {

      ## the name of the configuration file to search for in the software installation
      ## directory (and sub-folders)
      "filename": "{configuration file name}", ## e.g. "server.properties"

      ## the name of the service installation folder under /usr/local/lib
      ## (eskimo standard installation path)
      "filesystemService": "{folder name}", ## e.g. "kafka"

      ## the type of the property syntax
      ## Currently only "variable" supported
      "propertyType": "variable",

      ## The format of the property definition in the configuration file
      ## e.g. "{name}: {value}" or "{name}={value}"
      "propertyFormat": "property format",

      ## The prefix to use in the configuration file for comments
      "commentPrefix": "#",

      ## The list of properties to be editable by administrators using the eskimo UI
      "properties": [
        {

          ## name of the property
          "name": "{property name}", ## e.g. "num.network.threads"

          ## the description to show in the UI
          "comment": "{property description}",

          ## the default value to use if undefined by administrators
          "defaultValue": "{default property value}" ## e.g. "3"
        }
      ]
    }
  ],

  ## [optional] array of custom commands that are made available from the context menu
  ## on the System Status Page (when clicking on services status (OK/KO/etc.)
  "commands" : [
    {

      ## ID of the command. Needs to be a string with only [a-zA-Z_]
      "id" : "{command_id}", ## e.g. "show_log"

      ## Name of the command. This name is displayed in the menu
      "name" : "{command_name}", ## e.g. "Show Logs"

      ## The System command to be called on the node running the service
      "command": "{system_command}", ## e.g. "cat /var/log/ntp/ntp.log"

      ## The font-awesome icon to be displayed in the menu
      "icon": "{fa-icon}" ## e.g. "fa-file"
    }
  ]
}
----

(Bear in mind that since json actually doesn't support such thing as comments, the example above is actually not a valid
JSON snippet - comments starting with '##' would need to be removed.)

Everything is pretty straightforward and one should really look at the services pre-packaged within eskimo to get
inspiration when designing a new service to be operated by eskimo.


[[topology]]
==== Eskimo Topology and dependency management

As stated above, the most essential configuration property in a _service definition_ is the `masterElectionStrategy`
of a dependency. +
The whole master / slave topology management logic as well as the whole dependencies framework of eskimo relies on it.

==== Master Election strategy

Let's start by introducing what are the supported values for this `masterElectionStrategy` property:

* `NONE` : This is the simplest case. This enables a service to define as requiring another service without
bothering where it should be installed. It just has to be present somewhere on the cluster and the first service
doesn't care where. +
It however enforces the presence of that dependency service somewhere and refuses to validate the installation if the
dependency is not available somewhere on the eskimo nodes cluster.
* `FIRST_NODE` : This is used to define a simple dependency on another service. In addition, `FIRST_NODE` indicates that
the service where it is declared wants to know about at least one node where the dependency service is available. +
That other node should be the _first node_ found where that dependency service is available. +
_First_ node means that the nodes are processed by their order of declaration. The first node than runs the dependency
service will be given as dependency to the declaring service.
* `SAME_NODE_OR_RANDOM` : This is used to define a simple dependency on another service. In details,
`SAME_NODE_OR_RANDOM` indicates that the first service wants to know about at least one node where the dependency
service is available. +
In the case of `SAME_NODE_OR_RANDOM`, eskimo tries to find the depencency service on the very same node than
the one running the declaring service if that dependent service is available on that very same node. +
If no instance of the dependency service is not running on the very same node, then any other random node running the
dependency service is used as dependency.
* `RANDOM` : This is used to define a simple dependency on another service. In details, `RANDOM` indicates that the
first service wants to know about at least one node where the dependency service is available. That other node can be
any other node of the cluster where the dependency service is installed.
* `RANDOM_NODE_AFTER` : This is used to define a simple dependency on another service. In details, `RANDOM_NODE_AFTER`
indicates that the first service wants to know about at least one node where that dependency service is available. +
That other node should be any node of the cluster where the second service is installed yet with a *node number*
(internal eskimo node declaration order) greater than the current node where the first service is installed. +
This is usefull to define a chain of dependencies where every node instance depends on another node instance in a
circular way (pretty nifty for instance for elasticsearch discovery configuration).
* `SAME_NODE` : This means that the dependency service is expected to be available on the same node than the first
service, otherwise eskimo will report an error during service installation.

*The best way to understand this is to look at the exanples in eskimo pre-packaged services declared in the bundled
`services.json`.*

For instance:

* Cerebro tries to use the co-located instance of elasticsearch if it is available or any random one otherwise for
instance by using the following dependency declaration:

.cerebro dependency on elasticsearch
----
    "dependencies": [
      {
        "masterElectionStrategy": "SAME_NODE_OR_RANDOM",
        "masterService": "elasticsearch",
        "numberOfMasters": 1,
        "mandatory": true
      }
    ]
----

* elasticsearch instances on the different nodes search for each other in a round robin fashion by declating the
following dependencies (mandatory false ise used to support single node deployments):

.elasticsearch dependency on next elasticsearch instance
----
    "dependencies": [
      {
        "masterElectionStrategy": "RANDOM_NODE_AFTER",
        "masterService": "elasticsearch",
        "numberOfMasters": 1,
        "mandatory": false
      }
    ],
----

* logstash needs both elasticsearch and gluster. In contrary to elasticsearch, gluster is required on every node in a
multi-node cluster setup. Hence the following dependencies declaration for gluster:

.gluster dependencies definition
----
    "dependencies": [
      {
        "masterElectionStrategy": "SAME_NODE_OR_RANDOM",
        "masterService": "elasticsearch",
        "numberOfMasters": 1,
        "mandatory": true
      },
      {
        "masterElectionStrategy": "SAME_NODE",
        "masterService": "gluster",
        "numberOfMasters": 1,
        "mandatory": false
      }
----

* kafka uses zookeeper on the first node (in the order of declaration of nodes in the eskimo cluster) on which zookeeper
is available:

.kafka dependency on zookeeper
----
    "dependencies": [
      {
        "masterElectionStrategy": "FIRST_NODE",
        "masterService": "zookeeper",
        "numberOfMasters": 1,
        "mandatory": true
      }
----


Look at other examples to get inspired.

==== Memory allocation

Another pretty important property in a service configuration in `services.json` is the memory consumption property:
`memory`.

The possible values for that property are as follows :

* `neglectable` : the service is not accounted in memory allocation
* `small` : the service gets a single share of memory
* `medium` : the service gets two shares of memory
* `large` : the service gets three shares of memory

The system then works by computing the sum of shares for all nodes and then allocating the available memory on the node
to every service by dividing it amongst shares and allocating the corresponding portion of memory to every service. +
Of course, the system first removes from the available memory a significant portion to ensure some room for kernel and
filesystem cache.

===== Examples of memory allocation

Let's imagine the following services installed on a cluster node, along with their memory setting:

* *ntp* - neglectable
* *prometheus* - neglectable
* *gluster* - neglectable
* *mesos agent* - *verylarge*
* *elasticsearch* - large
* *logstash* - small
* *kafka* - medium
* *zookeeper* - neglectable

The following table gives various examples in terms of memory allocation for three different total RAM size values on the
cluster node running these services. +
The different columns gives how much memory is allocated to the different services in the different rows for various
size of total RAM.

[width="80%",frame="topbot",options="header"]
|===================
| Node total RAM                | 8 Gb  | 16 Gb | 20 Gb
| *ntp*                         |  -    |  -    |  -
| *prometheus*                  |  -    |  -    |  -
| *gluster*                     |  -    |  -    |  -
| *mesos agent*                 | 2500m | 5357m | 6786m
| *elasticsearch*               | 1500m | 3214m | 4071m
| *logstash*                    |  500m | 1071m | 1357m
| *kafka*                       | 1000m | 2143m | 2714m
| *zookeeper*                   |  -    |  -    |  -
| *_Filesystem cache reserve_*  | 1500m | 3214m | 4071m
| *_OS reserve_*                | 1000m | 1000m | 1000m
|===================

*Importantly*, all marathon services - such as Kibana, Cerebro, Kafka-manager, etc. - as well as all services operated by mesos -
such as the spark executors and flink workers - don't get any specific amount of memory assigned. +
*Instead, they share the memory available for mesos-agent.*


==== Topology file on cluster nodes

Every time the cluster nodes / services configuration is changed. Eskimo will verify the global services topology and
generate for every node of the cluster a "*topology definition file*".

That topology definition file defines all the dependencies and where to find them (using the notion of MASTER) for every
service running on every node.

The "topology definition file" can be fond on nodes in `/etc/eskimo_topology.sh`.

[[proxying]]
=== Proxying services web consoles

Many services managed by eskimo have web consoles used to administer them, such as mesos-agents, mesos-master,
kafka-manager, etc.
Some are even only web consoles used to administer other services or perform Data Science tasks, such as Kibana,
Zeppelin or GDash, etc.

Eskimo supports two modes for providing these web consoles in its own UI as presented in configuration above:

1. (A) Configuration of an `urlTemplate` which is used by eskimo to show an iframe displaying directly the web console
from the node on which it is installed. *This method is supported for backwards compatibility purpose but it is not
recommended*
2. (B) Configuration of a `proxyTargetPort` for full proxying and tnneling (using SSH) of the whole HTTP flow to the
web console using eskimo embedded proxying and tunneling feature. *This is the recommanded way* and this is the way
used by all eskimo pre-packaged services and web consoles.

Proxying works as explained  in the User Guide in the section "SSH Tunelling".

Proxying is however a little more complicated to set up since eskimo needs to perform a lot of rewriting on the text
resources (javascript, html and json) served by the proxied web console to rewrite served URLs to make them pass
through the proxy.

Eskimo provides a powerful rewrite engine that one can use to implement the rewrite rules defined in the configuration
as presented above.

==== Rewrite rules

Proxying web consoles HTTP flow means that a lot of the text resources served by the individual target web consoles
need to be processed in such a way that absolute URLs are rewritten.
This is unfortunately tricky and many different situations can occur, from URL build dynamically in javascript to static
resources URLs in CSS files for instance.

An eskimo service developer needs to analyze the application, debug it and understand every pattern that needs to be
replaced and define a *rewrite rule* for each of them.

==== Standard rewrite rules

A set of standard rewrite rules are implemented once and for all by the eskimo HTTP proxy for all services. By default
these standard rewrite rules are enabled for a service unless the service config declares
`"applyStandardProxyReplacements": false` in which case they are not applied to that specific service. +
This is usefull when a standard rule is actually harming a specific web console behaviour.

The standard rewrite rules are as follows:

.Standard rewrite rules
----

{
  "type" : "PLAIN",
  "source" : "src=\"/",
  "target" : "src=\"/{PREFIX_PATH}/"
},
{
  "type" : "PLAIN",
  "source" : "action=\"/",
  "target" : "action=\"/{PREFIX_PATH}/"
},
{
  "type" : "PLAIN",
  "source" : "href=\"/",
  "target" : "href=\"/{PREFIX_PATH}/"
},
{
  "type" : "PLAIN",
  "source" : "href='/",
  "target" : "href='/{PREFIX_PATH}/"
},
{
  "type" : "PLAIN",
  "source" : "url(\"/",
  "target" : "url(\"/{PREFIX_PATH}/"
},
{
  "type" : "PLAIN",
  "source" : "url('/",
  "target" : "url('/{PREFIX_PATH}/"
},
{
  "type" : "PLAIN",
  "source" : "url(/",
  "target" : "url(/{PREFIX_PATH}/"
},
{
  "type" : "PLAIN",
  "source" : "/api/v1",
  "target" : "/{PREFIX_PATH}/api/v1"
},
{
  "type" : "PLAIN",
  "source" : "\"/static/",
  "target" : "\"/{PREFIX_PATH}/static/"
},
----

==== Custom rewrite rules

In addition to the standard rewrite rules - that can be used or not by a service web console - an eskimo service
developer can define as many custom rewrite rules as he wants in the service configuration in `services.json` as
presented above.

Some patterns can be used in both the `source` and `target` strings that will be replaced by the framework before they
are searched, respectively injected, in the text stream:

* `CONTEXT_PATH` will be resolved by the context root at which the eskimo web application is deployed, such as for
instance `eskimo`
* `PREFIX_PATH` will be resolved by the specific context path of the service web console context, suchas for instance
for kibana `{CONTEXT_PATH}/kibana`, e.g. `eskimo/kibana` or `kibana` if no context root is used.
















// marker for exclusion : line 690



[appendix]
== Copyright and License


Eskimo is Copyright 2019 eskimo.sh - All rights reserved. +
Author : http://www.eskimo.sh

Eskimo is available under a dual licensing model : commercial and GNU AGPL. +
If you did not acquire a commercial licence for Eskimo, you can still use it and consider it free software under the
terms of the GNU Affero Public License. You can redistribute it and/or modify it under the terms of the GNU Affero
Public License  as published by the Free Software Foundation, either version 3 of the License, or (at your option)
any later version. +
Compliance to each and every aspect of the GNU Affero Public License is mandatory for users who did no acquire a
commercial license.

Eskimo is distributed as a free software under GNU AGPL in the hope that it will be useful, but WITHOUT ANY
WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
Affero Public License for more details.

You should have received a copy of the GNU Affero Public License along with Eskimo. If not,
see <https://www.gnu.org/licenses/> or write to the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
Boston, MA, 02110-1301 USA.

You can be released from the requirements of the license by purchasing a commercial license. Buying such a
commercial license is mandatory as soon as :

* you develop activities involving Eskimo without disclosing the source code of your own product, software, platform,
  use cases or scripts.
* you deploy eskimo as part of a commercial product, platform or software.

For more information, please contact eskimo.sh at https://www.eskimo.sh

The above copyright notice and this licensing notice shall be included in all copies or substantial portions of the
Software.