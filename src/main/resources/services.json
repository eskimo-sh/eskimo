{

  "ntp" : {
    "config": {
      "imageName" : "ntp",
      "order": 0,
      "mandatory": true,
      "name" : "NTP",
      "selectionLayout" : { "row" : 1, "col" : 1},
      "memory": "neglectable",
      "logo" : "images/ntp-logo.png",
      "icon" : "images/ntp-icon.png"
    },
    "dependencies": [
      {
        "masterElectionStrategy": "FIRST_NODE",
        "masterService": "ntp",
        "numberOfMasters": 1,
        "mandatory": true,
        "restart": true
      }
    ],
    "commands" : [
      {
        "id" : "show_log",
        "name" : "Show Logs",
        "command": "cat /var/log/ntp/ntp.log",
        "icon": "fa-file"
      }
    ]
  },



  "zookeeper": {
    "config": {
      "imageName" : "zookeeper",
      "order": 1,
      "unique": true,
      "name" : "Zookeeper",
      "selectionLayout" : { "row" : 1, "col" : 2},
      "memory": "small",
      "logo" : "images/zookeeper-logo.png",
      "icon" : "images/zookeeper-icon.png"
    },
    "dependencies": [
      {
        "masterElectionStrategy": "RANDOM",
        "masterService": "zookeeper",
        "numberOfMasters": 1,
        "mandatory": true,
        "restart": true
      }
    ],
    "additionalEnvironment": [ "SERVICE_NUMBER_1_BASED" ],
    "editableSettings": [
      {
        "filename": "environment",
        "propertyType": "REGEX",
        "propertyFormat": "{name}{value}",
        "commentPrefix": "#",
        "filesystemService": "zookeeper",
        "properties": [
          {
            "name": "Xms",
            "comment": "Startup Heap Size for zookeeper\n [ESKIMO_DEFAULT] means memory allocator will decide of zookeeper memory share.",
            "defaultValue": "[ESKIMO_DEFAULT]"
          },
          {
            "name": "Xmx",
            "comment": "Maximum Heap Size for zookeeper\n [ESKIMO_DEFAULT] means memory allocator will decide of zookeeper memory share.",
            "defaultValue": "[ESKIMO_DEFAULT]"
          }
        ]
      }
    ],
    "commands" : [
      {
        "id" : "show_log",
        "name" : "Show Logs",
        "command": "cat /var/log/zookeeper/zookeeper.log",
        "icon": "fa-file"
      }
    ]
  },



  "prometheus" : {
    "config": {
      "imageName" : "prometheus",
      "order": 2,
      "mandatory": true,
      "group" : "Monitoring",
      "name" : "Prometheus",
      "selectionLayout" : { "row" : 3, "col" : 1},
      "memory": "neglectable",
      "logo" : "images/prometheus-logo.png",
      "icon" : "images/prometheus-icon.png"
    },
    "dependencies": [
      {
        "masterElectionStrategy": "RANDOM",
        "masterService": "prometheus",
        "numberOfMasters": 1,
        "mandatory": true,
        "restart": true
      }
    ],
    "additionalEnvironment": [
      "ALL_NODES_LIST_prometheus"
    ],
    "commands" : [
      {
        "id" : "show_log",
        "name" : "Show Logs",
        "command": "cat /var/log/prometheus/prometheus.log",
        "icon": "fa-file"
      }
    ]
  },



  "grafana" : {
    "config": {
      "imageName" : "grafana",
      "order": 3,
      "unique": true,
      "kubernetes": true,
      "kubeConfig": {
        "request": {
          "cpu": "0.3",
          "ram": "400M"
        }
      },
      "group" : "Monitoring",
      "name" : "Grafana",
      "memory": "small",
      "logo" : "images/grafana-logo.png",
      "icon" : "images/grafana-icon.png"
    },
    "ui": {
      "proxyTargetPort" : 31300,
      "waitTime": 5000,
      "title" : "Grafana Monitoring",
      "role" : "*",
      "statusPageLinktitle" : "Access all monitoring dashboards in Grafana",
      "proxyReplacements" : [
        {
          "type" : "PLAIN",
          "source" : "{PREFIX_PATH}/{PREFIX_PATH}",
          "target" : "{PREFIX_PATH}"
        },
        {
          "comment": "for whatever reason, I need to replace this without grafana. The thing is that I am not the one putting it ?!?",
          "type" : "PLAIN",
          "source" : "{CONTEXT_PATH}api/v1",
          "target" : "api/v1"
        },
        {
          "comment": "for whatever reason, I need to replace this without grafana. The thing is that I am not the one putting it ?!?",
          "type" : "PLAIN",
          "source" : "{PREFIX_PATH}/api/v1",
          "target" : "api/v1"
        },
        {
          "type" : "PLAIN",
          "source" : "this._websocket(this._url)",
          "target" : "this._websocket(this._url.replace(\"ws://localhost:9191/grafana\", \"ws://localhost:9191/ws/grafana\"))"
        },
        {
          "type" : "PLAIN",
          "source" : "prometheus/api",
          "target" : "prometheus/grafana/api"
        }
      ]
    },
    "dependencies": [
      {
        "masterElectionStrategy": "RANDOM",
        "masterService": "prometheus",
        "numberOfMasters": 1,
        "mandatory": true,
        "restart": true
      }
    ],
    "editableSettings": [
      {
        "filename": "defaults.ini",
        "filesystemService": "grafana",
        "propertyType": "variable",
        "propertyFormat": "{name} = {value}",
        "commentPrefix": "#",
        "properties": [
          {
            "name": "admin_user",
            "comment": "Login Name of the Grafana admin user within eskimo\n<strong>CAN ONLY BE CHANGED BEFORE FIRST START OF GRAFANA</strong>",
            "defaultValue": "eskimo"
          },
          {
            "name": "admin_password",
            "comment": "Login Password of the Grafana admin user within eskimo\n<strong>CAN ONLY BE CHANGED BEFORE FIRST START OF GRAFANA</strong>",
            "defaultValue": "eskimo"
          }
        ]
      }
    ],
    "additionalEnvironment": [ "CONTEXT_PATH" ]
  },



  "gluster" : {
    "config": {
      "imageName" : "gluster",
      "order": 4,
      "mandatory": true,
      "name" : "Gluster / EGMI",
      "selectionLayout" : { "row" : 2, "col" : 1},
      "memory": "neglectable",
      "logo" : "images/gluster-logo.png",
      "icon" : "images/gluster-icon.png"
    },
    "ui": {
      "urlTemplate": "./gluster/{NODE_ADDRESS}/egmi/app.html",
      "proxyTargetPort": 28901,
      "waitTime": 10000,
      "role" : "ADMIN",
      "title": "Gluster Dashboard",
      "statusPageLinktitle": "Monitor Gluster volumes"
    },
    "masterDetection": {
      "strategy" : "LOG_FILE",
      "logFile" : "/var/log/gluster/egmi/egmi.log",
      "grep": "I am the new leader",
      "timeStampExtractRexp" : "([0-9\\-]+ [0-9.:,]+).*",
      "timeStampFormat" : "yyyy-MM-dd HH:mm:ss,SSS"
    },
    "dependencies": [
      {
        "masterElectionStrategy": "FIRST_NODE",
        "masterService": "zookeeper",
        "numberOfMasters": 1,
        "mandatory": true,
        "restart": true
      }
    ],
    "additionalEnvironment": [
      "ALL_NODES_LIST_gluster"
    ],
    "editableSettings": [
      {
        "filename": "egmi.properties",
        "filesystemService": "egmi",
        "propertyType": "variable",
        "propertyFormat": "{name}={value}",
        "commentPrefix": "#",
        "properties": [
          {
            "name": "target.volumes",
            "comment": "The volumes to be automagically managed by EGMI.",
            "defaultValue": "",
            "value": "spark_eventlog,spark_data,flink_data,kafka_data,flink_completed_jobs,logstash_data,kubernetes_registry,kubernetes_ssl"
          }
        ]
      }
    ],
    "commands" : [
      {
        "id" : "show_log",
        "name" : "Show GlusterFS Logs",
        "command": "sudo cat /var/log/gluster/glusterfs.log",
        "icon": "fa-file"
      },
      {
        "id" : "show_remote_log",
        "name" : "Show Remote Server Logs",
        "command": "cat /var/log/gluster/gluster_remote-log",
        "icon": "fa-file"
      }
    ]
  },



  "etcd": {
    "config": {
      "order": 5,
      "group" : "Kubernetes",
      "name" : "Etcd",
      "selectionLayout" : { "row" : 4, "col" : 1},
      "memory": "neglectable",
      "logo" : "images/kube-master-logo.png",
      "icon" : "images/kube-master-icon.png"
    },
    "dependencies": [
      {
        "masterElectionStrategy": "SAME_NODE",
        "masterService": "gluster",
        "numberOfMasters": 1,
        "mandatory": false,
        "restart": true
      }
    ],
    "additionalEnvironment": [
      "ALL_NODES_LIST_etcd",
      "SERVICE_NUMBER_1_BASED"
    ]
  },



  "kube-master": {
    "config": {
      "imageName" : "kube-master",
      "order": 6,
      "unique": true,
      "group" : "Kubernetes",
      "name" : "Master",
      "selectionLayout" : { "row" : 2, "col" : 2},
      "memory": "neglectable",
      "logo" : "images/kube-master-logo.png",
      "icon" : "images/kube-master-icon.png"
    },
    "dependencies": [
      {
        "masterElectionStrategy": "SAME_NODE",
        "masterService": "gluster",
        "numberOfMasters": 1,
        "mandatory": false,
        "restart": true
      },
      {
        "masterElectionStrategy": "FIRST_NODE",
        "masterService": "kube-master",
        "numberOfMasters": 1,
        "mandatory": true,
        "restart": true
      },
      {
        "masterElectionStrategy": "SAME_NODE",
        "masterService": "etcd",
        "numberOfMasters": 1,
        "mandatory": true,
        "restart": false
      }
    ]
  },



  "kube-slave": {
    "config": {
      "order": 7,
      "group" : "Kubernetes",
      "name" : "Slave",
      "selectionLayout" : { "row" : 5, "col" : 1},
      "memory": "NEGLECTABLE",
      "logo" : "images/kube-slave-logo.png",
      "icon" : "images/kube-slave-icon.png"
    },
    "dependencies": [
      {
        "masterElectionStrategy": "FIRST_NODE",
        "masterService": "kube-master",
        "numberOfMasters": 1,
        "mandatory": true,
        "restart": true
      },
      {
        "masterElectionStrategy": "SAME_NODE",
        "masterService": "gluster",
        "numberOfMasters": 1,
        "mandatory": true,
        "restart": true
      },
      {
        "masterElectionStrategy": "SAME_NODE",
        "masterService": "etcd",
        "numberOfMasters": 1,
        "mandatory": true,
        "restart": false
      }
    ],
    "additionalEnvironment": [
      "ALL_NODES_LIST_kube-slave",
      "SERVICE_NUMBER_1_BASED"
    ]
  },



  "kubernetes-dashboard" : {
    "config": {
      "imageName" : "kubernetes-dashboard",
      "order": 8,
      "kubernetes": true,
      "kubeConfig": {
        "request": {
          "cpu": "0.3",
          "ram": "500M"
        }
      },
      "unique": true,
      "group" : "Kubernetes",
      "name" : "Dashboard",
      "memory": "small",
      "logo" : "images/kube-master-logo.png",
      "icon" : "images/kube-master-icon.png"
    },
    "webCommands": [
      {
        "id" : "kubeDashboardLoginToken",
        "service": "kube-master",
        "command": "/usr/local/bin/kubectl get secret `/usr/local/bin/kubectl get sa/eskimo -o jsonpath=\"{.secrets[0].name}\"` -o go-template=\"{{.data.token | base64decode}}\""
      }
    ],
    "ui": {
      "__comment_urlTemplate": "Going through Kubectl proxy.",
      "urlTemplate": "kubernetes-dashboard/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/#/workloads?namespace=_all",
      "__comment_proxyTargetPort": "Kubectl proxy target port",
      "proxyTargetPort" : 8001,
      "waitTime": 5000,
      "role" : "ADMIN",
      "title" : "Kubernetes Dashboard",
      "statusPageLinktitle" : "Manage Kubernetes Cluster",
      "proxyReplacements" : [
        {
          "type" : "PLAIN",
          "source" : "h=\"https\"===h.slice(0,5)?\"wss\"+h.slice(5):\"ws\"+h.slice(4),this.url=h,",
          "target" : "h=\"https\"===h.slice(0,5)?\"wss\"+h.slice(5):\"ws\"+h.slice(4),this.url=h.replace(\"kubernetes-dashboard/api\",\"ws/kubernetes-dashboard/api\"),"
        },
        {
          "type" : "PLAIN",
          "source" : "\"WebSocket connection broken\"),d._cleanup()}}",
          "target" : "\"WebSocket connection broken\"),d._cleanup()};let eskThat=this;eskThat.ws.onopen = function (event) {eskThat.ws.send(\"HELLO_ESKIMO\");};}"
        },
        {
          "type" : "PLAIN",
          "source" : "kubernetes-dashboard//",
          "target" : "kubernetes-dashboard/"
        },
      ],
      "pageScripters" : [
        {
          "resourceUrl" : "api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/",
          "script": "function eskimoLoginChecker() {\n        \n    let contextPath = \"\";\n        \n    if (document.getElementsByTagName (\"kd-login\").length >= 1) {\n\n        document.getElementsByClassName(\"mat-radio-input\")[0].click();\n        \n        const fetchToken = async () => {\n            const response = await fetch((contextPath != \"\" ? \"/\" + contextPath + \"/\" : \"\") + \"/eskimo-command/kubeDashboardLoginToken\");\n            const result = await response.json(); //extract JSON from the http response\n\n            const error = result.error;\n            if (error && error != \"\") {\n                console.log (error);\n            } else {\n\n                const loginToken = result.value;\n\n                const tokenInput = document.getElementById(\"token\");\n                tokenInput.value = loginToken;\n                \n                if (\"createEvent\" in document) {\n                    var evt = document.createEvent(\"HTMLEvents\");\n                    evt.initEvent(\"change\", false, true);\n                    tokenInput.dispatchEvent(evt);\n                } else {\n                    tokenInput.fireEvent(\"onchange\");\n                }                \n        \n                setTimeout (function() { \n                    tokenInput.value = '';\n                    tokenInput.setAttribute('readonly','readonly');                \n                    document.getElementsByTagName(\"button\")[0].click(); \n                }, 50);\n            }\n        }    \n        \n        fetchToken();\n    }\n    \n    setTimeout (function() { eskimoLoginChecker();}, 3000);\n}\n\neskimoLoginChecker();"
        }
      ]
    }
  },



  "kafka" : {
    "config": {
      "imageName" : "kafka",
      "order": 9,
      "kubernetes": true,
      "kubeConfig": {
        "request": {
          "cpu": "1",
          "ram": "1G"
        }
      },
      "group" : "Kafka",
      "name" : "Broker",
      "memory": "medium",
      "logo" : "images/kafka-logo.png",
      "icon" : "images/kafka-icon.png"
    },
    "dependencies": [
      {
        "masterElectionStrategy": "FIRST_NODE",
        "masterService": "zookeeper",
        "numberOfMasters": 1,
        "mandatory": true,
        "restart": true
      },
      {
        "masterElectionStrategy": "ALL_NODES",
        "masterService": "kube-slave",
        "numberOfMasters": 1,
        "mandatory": true,
        "restart": true
      }
    ],
    "editableSettings": [
      {
        "filename": "eskimo-memory.opts",
        "propertyType": "REGEX",
        "propertyFormat": "{name}{value}",
        "commentPrefix": "#",
        "filesystemService": "kafka",
        "properties": [
          {
            "name": "Xms",
            "comment": "Startup Heap Size for Kafka java process\n [ESKIMO_DEFAULT] means memory allocator will decide of Kafka memory share.",
            "defaultValue": "[ESKIMO_DEFAULT]"
          },
          {
            "name": "Xmx",
            "comment": "Maximum Heap Size for Kafka java process\n [ESKIMO_DEFAULT] means memory allocator will decide of Kafka memory share.",
            "defaultValue": "[ESKIMO_DEFAULT]"
          }
        ]
      },
      {
        "filename": "server.properties",
        "filesystemService": "kafka",
        "propertyType": "variable",
        "propertyFormat": "{name}={value}",
        "commentPrefix": "#",
        "properties": [
          {
            "name": "num.network.threads",
            "comment": "The number of threads that the server uses for receiving requests from the network and sending responses to the network",
            "defaultValue": "3"
          },
          {
            "name": "num.io.threads",
            "comment": "The number of threads that the server uses for processing requests, which may include disk I/O",
            "defaultValue": "8"
          },
          {
            "name": "socket.send.buffer.bytes",
            "comment": "The send buffer (SO_SNDBUF) used by the socket server",
            "defaultValue": "102400"
          },
          {
            "name": "socket.receive.buffer.bytes",
            "comment": "The receive buffer (SO_RCVBUF) used by the socket server",
            "defaultValue": "102400"
          },
          {
            "name": "socket.request.max.bytes",
            "comment": "The maximum size of a request that the socket server will accept (protection against OOM)",
            "defaultValue": "104857600"
          },
          {
            "name": "num.partitions",
            "comment": "The default number of log partitions per topic. More partitions allow greater parallelism for consumption, but this will also result in more files across the brokers.",
            "defaultValue": "1"
          },
          {
            "name": "log.retention.hours",
            "comment": "The minimum age of a log file to be eligible for deletion due to age",
            "defaultValue": "168"
          }
        ]
      }
    ],
    "commands" : [
      {
        "id" : "show_server_log",
        "name" : "Show Server Logs",
        "command": "cat /var/log/kafka/server.log",
        "icon": "fa-file"
      },
      {
        "id" : "show_controller_log",
        "name" : "Show Controller Logs",
        "command": "cat /var/log/kafka/controller.log",
        "icon": "fa-file"
      },
      {
        "id" : "show_state_change_log",
        "name" : "Show State Change Logs",
        "command": "cat /var/log/kafka/state-change.log",
        "icon": "fa-file"
      },
      {
        "id" : "mount_kafka_data",
        "name" : "Mount kafka_data on Host",
        "command": "sudo /usr/local/sbin/gluster_mount.sh kafka_data /var/lib/kafka/data kafka",
        "icon": "fa-folder"
      }
    ]
  },



  "kafka-cli" : {
    "config": {
      "order": 10,
      "group": "Kafka",
      "name": "Client",
      "selectionLayout": {
        "row": 2,
        "col": 3
      },
      "memory": "neglectable",
      "logo" : "images/kafka-logo.png",
      "icon" : "images/kafka-icon.png"
    },
    "commands" : [
      {
        "id" : "mount_kafka_data_kafkacli",
        "name" : "Mount kafka_data on Host",
        "command": "sudo /usr/local/sbin/gluster_mount.sh kafka_data /var/lib/kafka/data kafka",
        "icon": "fa-folder"
      }
    ]
  },



  "kafka-manager": {
    "config": {
      "imageName" : "kafka-manager",
      "order": 11,
      "kubernetes": true,
      "kubeConfig": {
        "request": {
          "cpu": "0.3",
          "ram": "400M"
        }
      },
      "unique": true,
      "group" : "Kafka",
      "name" : "Manager",
      "memory": "small",
      "logo" : "images/kafka-manager-logo.png",
      "icon" : "images/kafka-manager-icon.png"
    },
    "ui": {
      "__comment_urlTemplate": "Going through Kubectl proxy.",
      "urlTemplate": "kafka-manager/api/v1/namespaces/default/services/kafka-manager:31220/proxy/",
      "__comment_proxyTargetPort": "Kubectl proxy target port",
      "proxyTargetPort" : 8001,
      "waitTime": 6000,
      "role" : "ADMIN",
      "title" : "Kafka manager",
      "statusPageLinktitle" : "Manage your kafka topics",
      "proxyReplacements" : [
        {
          "type" : "PLAIN",
          "source" : "{PREFIX_PATH}/{PREFIX_PATH}",
          "target" : "{PREFIX_PATH}"
        }
      ]
    },
    "dependencies": [
      {
        "masterElectionStrategy": "FIRST_NODE",
        "masterService": "zookeeper",
        "numberOfMasters": 1,
        "mandatory": true,
        "restart": true
      },
      {
        "masterElectionStrategy": "RANDOM",
        "masterService": "kafka",
        "numberOfMasters": 1,
        "mandatory": true,
        "restart": false
      }
    ],
    "commands" : [
      {
        "id" : "show_log",
        "name" : "Show Logs",
        "command": "cat /var/log/kafka/kafka-manager/application.log",
        "icon": "fa-file"
      },
      {
        "id" : "mount_kafka_data_kafkamgr",
        "name" : "Mount kafka_data on Host",
        "command": "sudo /usr/local/sbin/gluster_mount.sh kafka_data /var/lib/kafka/data kafka",
        "icon": "fa-folder"
      }
    ]
  },



  "spark-history-server" : {
    "config": {
      "imageName" : "spark",
      "order": 12,
      "kubernetes": true,
      "kubeConfig": {
        "request": {
          "cpu": "0.3",
          "ram": "400M"
        }
      },
      "unique": true,
      "group" : "Spark",
      "name" : "Console",
      "memory": "small",
      "logo" : "images/spark-history-server-logo.png",
      "icon" : "images/spark-history-server-icon.png"
    },
    "ui": {
      "proxyTargetPort" : 31811,
      "waitTime": 5000,
      "role" : "*",
      "title" : "Spark Console",
      "statusPageLinktitle" : "Monitor your Spark jobs",
      "applyStandardProxyReplacements": false
    },
    "dependencies": [
      {
        "masterElectionStrategy": "RANDOM",
        "masterService": "gluster",
        "numberOfMasters": 1,
        "mandatory": false,
        "restart": true
      }
    ],
    "commands" : [
      {
        "id" : "show_log",
        "name" : "Show Logs",
        "command": "sudo cat /var/log/spark/`ls -t /var/log/spark/ | grep HistoryServer | head -n 1`",
        "icon": "fa-file"
      },
      {
        "id" : "mount_spark_eventlog_sparkhisto",
        "name" : "Mount spark_eventlog on Host",
        "command": "sudo /usr/local/sbin/gluster_mount.sh spark_eventlog /var/lib/spark/eventlog spark",
        "icon": "fa-folder"
      },
      {
        "id" : "mount_spark_data_sparkhisto",
        "name" : "Mount spark_data on Host",
        "command": "sudo /usr/local/sbin/gluster_mount.sh spark_data /var/lib/spark/data spark",
        "icon": "fa-folder"
      }
    ]
  },



  "spark-runtime" : {
    "config": {
      "imageName" : "spark",
      "order": 13,
      "group" : "Spark",
      "name" : "Runtime",
      "kubernetes": true,
      "kubeConfig": {
        "request": {
          "cpu": "1",
          "ram": "800M"
        }
      },
      "registryOnly": true,
      "memory": "large",
      "logo" : "images/spark-runtime-logo.png",
      "icon" : "images/spark-runtime-icon.png"
    },
    "dependencies": [
      {
        "masterElectionStrategy": "FIRST_NODE",
        "masterService": "kube-master",
        "numberOfMasters": 1,
        "mandatory": true,
        "restart": false
      }
    ],
    "editableSettings": [
      {
        "filename": "spark-defaults.conf",
        "propertyType": "variable",
        "propertyFormat": "{name}={value}",
        "commentPrefix": "#",
        "filesystemService": "spark",
        "properties" : [
          {
            "name" :  "spark.driver.memory",
            "comment": "Limiting the driver (client) memory",
            "defaultValue" : "800m"
          },
          {
            "name" :  "spark.rpc.numRetries",
            "comment": "Number of times to retry before an RPC task gives up. An RPC task will run at most times of this number.",
            "defaultValue" : "5"
          },
          {
            "name" :  "spark.rpc.retry.wait",
            "comment": "Duration for an RPC ask operation to wait before retrying.",
            "defaultValue" : "5s"
          },
          {
            "name" :  "spark.scheduler.mode",
            "comment": "The scheduling mode between jobs submitted to the same SparkContext. \nCan be FIFO or FAIR. FAIR Seem not to work well with Kubernetes",
            "defaultValue" : "FAIR"
          },
          {
            "name" :  "spark.locality.wait",
            "comment": "How long to wait to launch a data-local task before giving up and launching it on a less-local node.",
            "defaultValue" : "20s"
          },
          {
            "name" :  "spark.dynamicAllocation.executorIdleTimeout",
            "comment": "If dynamic allocation is enabled and an executor has been idle for more than this duration, the executor will be removed. \n (Caution here : small values cause issues. I have executors killed with 10s for instance)",
            "defaultValue" : "200s"
          },
          {
            "name" :  "spark.dynamicAllocation.cachedExecutorIdleTimeout",
            "comment": "If dynamic allocation is enabled and an executor which has cached data blocks has been idle for more than this duration, the executor will be removed. \n (Caution here : small values cause issues. I have executors killed with 10s for instance)",
            "defaultValue" : "300s"
          },
          {
            "name" :  "spark.dynamicAllocation.shuffleTracking.timeout",
            "comment": "When shuffle tracking is enabled, controls the timeout for executors that are holding shuffle data.",
            "defaultValue" : "600s"
          },
          {
            "name" :  "spark.dynamicAllocation.schedulerBacklogTimeout",
            "comment": "\tIf dynamic allocation is enabled and there have been pending tasks backlogged for more than this duration, new executors will be requested.",
            "defaultValue" : "5s"
          },
          {
            "name" :  "spark.executor.memory",
            "comment": "Defining default Spark executor memory allowed by Eskimo Memory Management (found in topology). \nUSE [ESKIMO_DEFAULT] to leave untouched or e.g. 800m, 1.2g, etc.",
            "defaultValue" : "[ESKIMO_DEFAULT]"
          }
        ]
      }
    ],
    "commands" : [
      {
        "id" : "mount_spark_eventlog",
        "name" : "Mount spark_eventlog on Host",
        "command": "sudo /usr/local/sbin/gluster_mount.sh spark_eventlog /var/lib/spark/eventlog spark",
        "icon": "fa-folder"
      },
      {
        "id" : "mount_spark_data",
        "name" : "Mount spark_data on Host",
        "command": "sudo /usr/local/sbin/gluster_mount.sh spark_data /var/lib/spark/data spark",
        "icon": "fa-folder"
      }
    ]
  },



  "spark-cli" : {
    "config": {
      "order": 14,
      "group" : "Spark",
      "name": "Client",
      "selectionLayout": {
        "row": 3,
        "col": 3
      },
      "memory": "neglectable",
      "logo" : "images/spark-runtime-logo.png",
      "icon" : "images/spark-runtime-icon.png"
    },
    "commands" : [
      {
        "id" : "mount_spark_eventlog_sparkcli",
        "name" : "Mount spark_eventlog on Host",
        "command": "sudo /usr/local/sbin/gluster_mount.sh spark_eventlog /var/lib/spark/eventlog spark",
        "icon": "fa-folder"
      },
      {
        "id" : "mount_spark_data_sparkcli",
        "name" : "Mount spark_data on Host",
        "command": "sudo /usr/local/sbin/gluster_mount.sh spark_data /var/lib/spark/data spark",
        "icon": "fa-folder"
      }
    ]
  },



  "flink-runtime" : {
    "config": {
      "imageName" : "flink",
      "order": 15,
      "group" : "Flink",
      "name" : "Runtime",
      "kubernetes": true,
      "kubeConfig": {
        "request": {
          "cpu": "1",
          "ram": "1G"
        }
      },
      "memory": "large",
      "logo" : "images/flink-runtime-logo.png",
      "icon" : "images/flink-runtime-icon.png"
    },
    "ui": {
      "__comment_urlTemplate": "Going through Kubectl proxy.",
      "urlTemplate": "flink-runtime/{NODE_ADDRESS}/api/v1/namespaces/default/services/flink-runtime-rest:8081/proxy/#/overview",
      "__comment_proxyTargetPort": "Kubectl proxy target port",
      "proxyTargetPort" : 8001,
      "waitTime": 5000,
      "role" : "*",
      "title" : "Flink Dashboard",
      "statusPageLinktitle" : "Manage Flink Session"
    },
    "masterDetection": {
      "strategy" : "LOG_FILE",
      "logFile" : "/var/log/flink/flink-log-kubernetes-jobmanager.sh.log",
      "grep": "Starting the resource manager.",
      "timeStampExtractRexp" : "([0-9\\-]+ [0-9.:,]+).*",
      "timeStampFormat" : "yyyy-MM-dd HH:mm:ss,SSS"
    },
    "dependencies": [
      {
        "masterElectionStrategy": "FIRST_NODE",
        "masterService": "zookeeper",
        "numberOfMasters": 1,
        "mandatory": true,
        "restart": true
      },
      {
        "masterElectionStrategy": "FIRST_NODE",
        "masterService": "kube-master",
        "numberOfMasters": 1,
        "mandatory": true,
        "restart": false
      }
    ],
    "editableSettings": [
      {
        "filename": "flink-conf.yaml",
        "propertyType": "variable",
        "propertyFormat": "{name}: {value}",
        "commentPrefix": "#",
        "filesystemService": "flink",
        "properties": [
          {
            "name": "jobmanager.memory.process.size",
            "comment": "The total process memory size for the JobManager. Use [ESKIMO_DEFAULT] to use Eskimo computed memory from memory allocation policy.",
            "defaultValue": "[ESKIMO_DEFAULT]"
          },
          {
            "name": "taskmanager.memory.process.size:",
            "comment": "The total process memory size for the TaskManager. Use [ESKIMO_DEFAULT] to use Eskimo computed memory from memory allocation policy.",
            "defaultValue": "[ESKIMO_DEFAULT]"
          },
          {
            "name": "parallelism.default",
            "comment": "Default parallelism for jobs. Default value : 1.",
            "defaultValue": "1"
          },
          {
            "name": "taskmanager.numberOfTaskSlots",
            "comment": "The number of parallel operator or user function instances that a single TaskManager can run. Default value : 1.",
            "defaultValue": "1"
          }
        ]
      }
    ],
    "commands" : [
      {
        "id" : "show_log",
        "name" : "Show Logs",
        "command": "sudo cat /var/log/flink/`ls -t /var/log/flink/ | grep flink- | head -n 1`",
        "icon": "fa-file"
      },
      {
        "id" : "mount_flink_data",
        "name" : "Mount flink_data on Host",
        "command": "sudo /usr/local/sbin/gluster_mount.sh flink_data /var/lib/flink/data flink",
        "icon": "fa-folder"
      },
      {
        "id" : "mount_flink_completed_jobs",
        "name" : "Mount flink_completed_jobs on Host",
        "command": "sudo /usr/local/sbin/gluster_mount.sh flink_completed_jobs /var/lib/flink/completed_jobs flink",
        "icon": "fa-folder"
      }
    ]
  },



  "flink-cli" : {
    "config": {
      "order": 16,
      "group" : "Flink",
      "name": "Client",
      "selectionLayout": {
        "row": 4,
        "col": 3
      },
      "memory": "neglectable",
      "logo" : "images/flink-logo.png",
      "icon" : "images/flink-icon.png"
    },
    "commands" : [
      {
        "id" : "mount_flink_data_flinkcli",
        "name" : "Mount flink_data on Host",
        "command": "sudo /usr/local/sbin/gluster_mount.sh flink_data /var/lib/flink/data flink",
        "icon": "fa-folder"
      },
      {
        "id" : "mount_flink_completed_jobs_flinkcli",
        "name" : "Mount flink_completed_jobs on Host",
        "command": "sudo /usr/local/sbin/gluster_mount.sh flink_completed_jobs /var/lib/flink/completed_jobs flink",
        "icon": "fa-folder"
      }
    ]
  },



  "logstash" : {
    "config": {
      "imageName" : "logstash",
      "order": 17,
      "kubernetes": true,
      "group" : "Elastic Stack",
      "name" : "Logstash",
      "memory": "small",
      "logo" : "images/logstash-logo.png",
      "icon" : "images/logstash-icon.png"
    },
    "dependencies": [
      {
        "masterElectionStrategy": "RANDOM",
        "masterService": "gluster",
        "numberOfMasters": 1,
        "mandatory": false,
        "restart": true
      },
      {
        "masterElectionStrategy": "ALL_NODES",
        "masterService": "kube-slave",
        "numberOfMasters": 1,
        "mandatory": true,
        "restart": true
      }
    ],
    "editableSettings": [
      {
        "filename": "jvm.options",
        "propertyType": "REGEX",
        "propertyFormat": "{name}{value}",
        "commentPrefix": "#",
        "filesystemService": "logstash",
        "properties": [
          {
            "name": "Xms",
            "comment": "Startup Heap Size for logstash process\n [ESKIMO_DEFAULT] means memory allocator will decide of logstash memory share.",
            "defaultValue": "[ESKIMO_DEFAULT]"
          },
          {
            "name": "Xmx",
            "comment": "Maximum Heap Size for logstash process\n [ESKIMO_DEFAULT] means memory allocator will decide of logstash memory share.",
            "defaultValue": "[ESKIMO_DEFAULT]"
          }
        ]
      }
    ],
    "commands" : [
      {
        "id" : "show_remote_log",
        "name" : "Show Remote Server Logs",
        "command": "cat /var/log/elasticsearch/logstash/logstash_remote.log",
        "icon": "fa-file"
      },
      {
        "id" : "mount_logstash_data",
        "name" : "Mount logstash_data on Host",
        "command": "sudo /usr/local/sbin/gluster_mount.sh logstash_data /var/lib/elasticsearch/logstash/data elasticsearch",
        "icon": "fa-folder"
      }
    ]
  },



  "logstash-cli" : {
    "config": {
      "order": 18,
      "group": "Elastic Stack",
      "name": "LS Client",
      "selectionLayout": {
        "row": 1,
        "col": 3
      },
      "memory": "neglectable",
      "logo": "images/logstash-logo.png",
      "icon": "images/logstash-icon.png"
    },
    "commands" : [
      {
        "id" : "mount_logstash_data_logstashcli",
        "name" : "Mount logstash_data on Host",
        "command": "sudo /usr/local/sbin/gluster_mount.sh logstash_data /var/lib/elasticsearch/logstash/data elasticsearch",
        "icon": "fa-folder"
      }
    ]
  },



  "cerebro" : {
    "config": {
      "imageName" : "cerebro",
      "order": 19,
      "kubernetes": true,
      "kubeConfig": {
        "request": {
          "cpu": "0.3",
          "ram": "400M"
        }
      },
      "unique": true,
      "group" : "Elastic Stack",
      "name" : "Cerebro",
      "memory": "small",
      "logo" : "images/cerebro-logo.png",
      "icon" : "images/cerebro-icon.png"
    },
    "ui": {
      "__comment_urlTemplate": "Going through Kubectl proxy.",
      "urlTemplate": "cerebro/api/v1/namespaces/default/services/cerebro:31900/proxy/#/overview?host=http:%2F%2Felasticsearch.default.svc.cluster.eskimo:9200",
      "__comment_proxyTargetPort": "Kubectl proxy target port",
      "proxyTargetPort" : 8001,
      "waitTime": 5000,
      "role" : "*",
      "title" : "Cerebro",
      "statusPageLinktitle" : "Manage your data in Elasticsearch"
    },
    "dependencies": [
      {
        "masterElectionStrategy": "RANDOM",
        "masterService": "elasticsearch",
        "numberOfMasters": 1,
        "mandatory": true,
        "restart": false
      }
    ],
    "editableSettings": [
      {
        "filename": "JVM_OPTS.sh",
        "propertyType": "REGEX",
        "propertyFormat": "{name}{value}",
        "commentPrefix": "#",
        "filesystemService": "cerebro",
        "properties": [
          {
            "name": "Xms",
            "comment": "Startup Heap Size for Cerebro java process\n[ESKIMO_DEFAULT] means memory allocator will decide of Cerebro memory share.",
            "defaultValue": "[ESKIMO_DEFAULT]"
          },
          {
            "name": "Xmx",
            "comment": "Maximum Heap Size for Cerebro java process\n[ESKIMO_DEFAULT] means memory allocator will decide of Cerebro memory share.",
            "defaultValue": "[ESKIMO_DEFAULT]"
          }
        ]
      }
    ]
  },



  "elasticsearch": {
    "config": {
      "imageName" : "elasticsearch",
      "order": 20,
      "kubernetes": true,
      "kubeConfig": {
        "request": {
          "cpu": "1",
          "ram": "1G"
        }
      },
      "group" : "Elastic Stack",
      "name" : "Elastic-Search",
      "memory": "large",
      "logo" : "images/elasticsearch-logo.png",
      "icon" : "images/elasticsearch-icon.png"
    },
    "dependencies": [
      {
        "masterElectionStrategy": "ALL_NODES",
        "masterService": "kube-slave",
        "numberOfMasters": 1,
        "mandatory": true,
        "restart": true
      }
    ],
    "editableSettings": [
      {
        "filename": "eskimo.options",
        "propertyType": "REGEX",
        "propertyFormat": "{name}{value}",
        "commentPrefix": "#",
        "filesystemService": "elasticsearch",
        "properties": [
          {
            "name": "Xms",
            "comment": "Startup Heap Size for ES java process\n[ESKIMO_DEFAULT] means memory allocator will decide of ES memory share.\n<strong>THIS NEEDS TO BE THE SAME VALUE AS Xmx OTHERWISE ES WILL LIKELY REFUSE TO START</strong>.",
            "defaultValue": "[ESKIMO_DEFAULT]"
          },
          {
            "name": "Xmx",
            "comment": "Maximum Heap Size for ES java process\n[ESKIMO_DEFAULT] means memory allocator will decide of ES memory share.\n<strong>THIS NEEDS TO BE THE SAME VALUE AS Xms OTHERWISE ES WILL LIKELY REFUSE TO START</strong>.",
            "defaultValue": "[ESKIMO_DEFAULT]"
          }
        ]
      },
      {
        "filename": "elasticsearch.yml",
        "propertyType": "variable",
        "propertyFormat": "{name}: {value}",
        "commentPrefix": "#",
        "filesystemService": "elasticsearch",
        "properties": [
          {
            "name": "action.destructive_requires_name",
            "comment": "Require explicit names when deleting indices",
            "defaultValue": "false"
          }
        ]
      },
      {
        "filename": "elasticsearch-index-defaults.properties",
        "propertyType": "variable",
        "propertyFormat": "{name}={value}",
        "commentPrefix": "#",
        "filesystemService": "elasticsearch",
        "properties" : [
          {
            "name" :  "index.refresh_interval",
            "comment": "Default refresh interval on new indices. Use format such as 10s, 1m, etc. \nUSE [ESKIMO_DEFAULT] to leave ElasticSearch default value.",
            "defaultValue" : "[ESKIMO_DEFAULT]"
          },
          {
            "name" :  "index.number_of_replicas",
            "comment": "Default number of additional replicas on new indices.  \nUSE [ESKIMO_DEFAULT] to let Eskimo compute best value.",
            "defaultValue" : "[ESKIMO_DEFAULT]"
          },
          {
            "name" :  "index.number_of_shards",
            "comment": "Default number of additional replicas on new indices.  \nUSE [ESKIMO_DEFAULT] to leave ElasticSearch default value (5).",
            "defaultValue" : "[ESKIMO_DEFAULT]"
          }
        ]
      }
    ],
    "commands" : [
      {
        "id" : "show_log",
        "name" : "Show Logs",
        "command": "cat /var/log/elasticsearch/eskimo.log",
        "icon": "fa-file"
      },
      {
        "id" : "show_search_slow_log",
        "name" : "Show Search Slow Logs",
        "command": "cat /var/log/elasticsearch/eskimo_index_search_slowlog.log",
        "icon": "fa-file"
      },
      {
        "id" : "show_index_slow_log",
        "name" : "Show Indexing Slow Logs",
        "command": "cat /var/log/elasticsearch/eskimo_index_indexing_slowlog.log",
        "icon": "fa-file"
      },
      {
        "id" : "show_gc_log",
        "name" : "Show GC Logs",
        "command": "cat /var/log/elasticsearch/gc.log",
        "icon": "fa-file"
      }
    ]
  },



  "kibana" : {
    "config": {
      "imageName" : "kibana",
      "order": 21,
      "kubernetes": true,
      "kubeConfig": {
        "request": {
          "cpu": "0.4",
          "ram": "600M"
        }
      },
      "unique": true,
      "group" : "Elastic Stack",
      "name" : "Kibana",
      "memory": "medium",
      "logo" : "images/kibana-logo.png",
      "icon" : "images/kibana-icon.png"
    },
    "ui": {
      "urlTemplate": "./kibana/app/home",
      "proxyTargetPort" : 31562,
      "waitTime": 8000,
      "role" : "*",
      "title" : "Kibana",
      "applyStandardProxyReplacements": false,
      "statusPageLinktitle" : "Visualize your data in Elasticsearch",
      "proxyReplacements" : [
        {
          "type" : "PLAIN",
          "source" : "'/kibana",
          "target" : "'/{PREFIX_PATH}"
        },
        {
          "type" : "PLAIN",
          "source" : "\"/kibana",
          "target" : "\"/{PREFIX_PATH}"
        },
        {
          "type" : "PLAIN",
          "source" : "&quot;/kibana",
          "target" : "&quot;/{PREFIX_PATH}"
        },
        {
          "type" : "PLAIN",
          "source" : "url(/kibana",
          "target" : "url(/{PREFIX_PATH}"
        }
      ]
    },
    "dependencies": [
      {
        "masterElectionStrategy": "RANDOM",
        "masterService": "elasticsearch",
        "numberOfMasters": 1,
        "mandatory": true,
        "restart": true
      },
      {
        "masterElectionStrategy": "FIRST_NODE",
        "masterService": "kube-master",
        "numberOfMasters": 1,
        "mandatory": true,
        "restart": false
      }
    ],
    "editableSettings" : [
      {
        "filename": "node.options",
        "filesystemService": "kibana",
        "propertyType": "REGEX",
        "propertyFormat": "{name}={value}",
        "commentPrefix": "#",
        "properties": [
          {
            "name": "max-old-space-size",
            "comment": "Maximum Old Space Size of the nodejs runtime for Kibana",
            "defaultValue": "[ESKIMO_DEFAULT]"
          }
        ]
      }
    ],
    "commands" : [
      {
        "id" : "show_log",
        "name" : "Show Logs",
        "command": "cat /var/log/elasticsearch/kibana/kibana.log",
        "icon": "fa-file"
      }
    ]
  },



  "zeppelin" : {
    "config": {
      "imageName" : "zeppelin",
      "order": 22,
      "unique": true,
      "kubernetes": true,
      "kubeConfig": {
        "request": {
          "cpu": "1",
          "ram": "1G"
        }
      },
      "name" : "Zeppelin",
      "memory": "verylarge",
      "____comment": "We need to know about the spark executor memory to set",
      "memoryAdditional__commentedOut": ["spark-runtime"],
      "logo" : "images/zeppelin-logo.png",
      "icon" : "images/zeppelin-icon.png"
    },
    "ui": {
      "__comment_urlTemplate": "Going through Kubectl proxy.",
      "urlTemplate": "zeppelin/api/v1/namespaces/default/services/zeppelin:31008/proxy/#/",
      "__comment_proxyTargetPort": "Kubectl proxy target port",
      "proxyTargetPort" : 8001,
      "waitTime": 8000,
      "role" : "*",
      "title" : "Zeppelin",
      "statusPageLinktitle" : "Use Zeppelin for your Data Science projects",
      "proxyReplacements" : [
        {
          "type" : "PLAIN",
          "source" : "return t+\"//\"+location.hostname+\":\"+this.getPort()+e(location.pathname)+\"/ws",
          "target" : "return t + \"//\" + location.hostname + \":\" + this.getPort() + \"/{CONTEXT_PATH}ws\" + e(location.pathname) + \"/ws"
        },
        {
          "type" : "PLAIN",
          "source" : "!function(e){var t={};",
          "target" : "function noOp(){}; !function(e){var t={};"
        },
        {
          "type" : "PLAIN",
          "source" : "console.log(\"Send",
          "target" : "noOp(\"Send"
        },
        {
          "type" : "PLAIN",
          "source" : "console.log(\"Receive",
          "target" : "noOp(\"Receive"
        },
        {
          "type" : "PLAIN",
          "source" : "<li><a href=\"/zeppelin/next\">Try the new Zeppelin</a></li>",
          "target" : ""
        }
      ]
    },
    "dependencies": [
      {
        "masterElectionStrategy": "RANDOM",
        "masterService": "elasticsearch",
        "numberOfMasters": 1,
        "mandatory": false,
        "restart": false
      },
      {
        "masterElectionStrategy": "FIRST_NODE",
        "masterService": "zookeeper",
        "numberOfMasters": 1,
        "mandatory": true,
        "restart": true
      },
      {
        "masterElectionStrategy": "RANDOM",
        "masterService": "gluster",
        "numberOfMasters": 1,
        "mandatory": false,
        "restart": true
      },
      {
        "masterElectionStrategy": "RANDOM",
        "masterService": "logstash",
        "numberOfMasters": 1,
        "mandatory": false,
        "restart": true
      },
      {
        "masterElectionStrategy": "RANDOM",
        "masterService": "kafka",
        "numberOfMasters": 1,
        "mandatory": false,
        "restart": false
      }
    ],
    "editableSettings": [
      {
        "filename": "zeppelin-env.sh",
        "filesystemService": "zeppelin",
        "propertyType": "REGEX",
        "propertyFormat": "{name}{value}",
        "commentPrefix": "#",
        "properties": [
          {
            "name": "Xmx",
            "comment": "Maximum heap size memory allocated to Zeppelin process.\n[ESKIMO_DEFAULT] leaves the default memory allocation strategy decide of it.",
            "defaultValue": "[ESKIMO_DEFAULT]"
          }
        ]
      },
      {
        "filename": "eskimo_settings.conf",
        "filesystemService": "zeppelin",
        "propertyType": "variable",
        "propertyFormat": "{name}={value}",
        "commentPrefix": "#",
        "properties": [
          {
            "name": "zeppelin_note_isolation",
            "comment": "The setting 'zeppelin_note_isolation' is used to control whether interpreter processes are created and managed globally for the whole zeppelin process or per note.\nPossible values are:\n 'shared' : one single instance of every interpreter is created and shared among users and notes (better for laboratory).\n 'per_note' : one instance of interpreter is created for every note (better for production - but requires a lot of RAM).\n",
            "defaultValue": "shared"
          }
        ]
      }
    ],
    "commands" : [
      {
        "id" : "show_log",
        "name" : "Show Logs",
        "command": "sudo cat /var/log/spark/zeppelin/`ls -t /var/log/spark/zeppelin/ | grep zeppelin-eskimo- | head -n 1`",
        "icon": "fa-file"
      },
      {
        "id" : "show_shell_log",
        "name" : "Show Shell Int. Logs",
        "command": "sudo cat /var/log/spark/zeppelin/`ls -t /var/log/spark/zeppelin/ | grep zeppelin-interpreter-sh | head -n 1`",
        "icon": "fa-file"
      },
      {
        "id" : "show_python_log",
        "name" : "Show Python Int. Logs",
        "command": "sudo cat /var/log/spark/zeppelin/`ls -t /var/log/spark/zeppelin/ | grep zeppelin-interpreter-python | head -n 1`",
        "icon": "fa-file"
      },
      {
        "id" : "show_spark_log",
        "name" : "Show Spark Int. Logs",
        "command": "sudo cat /var/log/spark/zeppelin/`ls -t /var/log/spark/zeppelin/ | grep zeppelin-interpreter-spark | head -n 1`",
        "icon": "fa-file"
      },
      {
        "id" : "show_flink_log",
        "name" : "Show Flink Int. Logs",
        "command": "sudo cat /var/log/spark/zeppelin/`ls -t /var/log/spark/zeppelin/ | grep zeppelin-interpreter-flink | head -n 1`",
        "icon": "fa-file"
      }
    ]
  }

}