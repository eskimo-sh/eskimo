#Activating EventLog stuff
spark.eventLog.enabled=true
spark.eventLog.dir=/var/lib/spark/eventlog

#Default serializer
spark.serializer=org.apache.spark.serializer.KryoSerializer

#Limiting the driver (client) memory
spark.driver.memory=800m

#This seems to help spark messing with hostnames instead of adresses and really helps
spark.driver.host=192.168.10.11
spark.driver.bindAddress=0.0.0.0

# Number of times to retry before an RPC task gives up. 
#An RPC task will run at most times of this number.
spark.rpc.numRetries=5

# Duration for an RPC ask operation to wait before retrying.
spark.rpc.retry.wait=5s

#Settings required for Spark driver distribution over mesos cluster (Cluster Mode through Mesos Dispatcher)
spark.mesos.executor.home=/usr/local/lib/spark/

#If set to true, runs over Mesos clusters in coarse-grained sharing mode, 
#where Spark acquires one long-lived Mesos task on each machine. 
#If set to false, runs over Mesos cluster in fine-grained sharing mode,
#where one Mesos task is created per Spark task.
#(Fine grained mode is deprecated and one should consider dynamic allocation instead)
spark.mesos.coarse=true

#ElasticSearch setting (first node to be reached => can use localhost eerywhere)
spark.es.nodes=localhost
spark.es.port=9200

#The scheduling mode between jobs submitted to the same SparkContext.
#Can be FIFO or FAIR. FAIR Seem not to work well with mesos
#(FIFO is the default BTW ...)
spark.scheduler.mode=FAIR

#How long to wait to launch a data-local task before giving up and launching it on a less-local node.
spark.locality.wait=20s

# Configuring dynamic allocation
# (See Spark configuration page online for more information)
spark.dynamicAllocation.enabled=true
#(Caution here : small values cause issues. I have executors killed with 10s for instance)
#spark.dynamicAllocation.executorIdleTimeout=200s
spark.dynamicAllocation.cachedExecutorIdleTimeout=300s

# Configuring spark shuffle service (required for dynamic allocation)
spark.shuffle.service.enabled=true

# Directory to use for scratch space in Spark, including map output files and RDDs that get stored on disk. 
# Spark Mesos Shuffle service and spark executors need to have acess to the same folder there cross containers. 
spark.local.dir=/var/lib/spark/tmp/

#Defining docker image to be used for spark executors
spark.mesos.executor.docker.image=kubernetes.registry:5000/spark-runtime
spark.mesos.executor.docker.parameters.network=host
spark.mesos.executor.docker.volumes=/var/log/spark:/var/log/spark:rw,/var/lib/spark:/var/lib/spark:rw

#For the filesystem history provider, the directory containing application event logs to load.
spark.history.fs.logDirectory=file:///var/lib/spark/eventlog

#The period at which to check for new or updated logs in the log directory.
spark.history.fs.update.interval=5s

#Finding the mesos master through zookeeper
spark.master=mesos://zk://192.168.10.11:2181/mesos

#Defining default Spark executor memory allowed by Eskimo Memory Management (found in topology)
spark.executor.memory=1872m
