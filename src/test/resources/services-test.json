{

  "distributed-time" : {
    "config": {
      "imageName" : "distributed-time",
      "order": 0,
      "mandatory": true,
      "conditional" : "MULTIPLE_NODES",
      "name" : "Distributed Time",
      "selectionLayout" : { "row" : 1, "col" : 1},
      "memory": "negligible",
      "logo" : "images/distributed-time-logo.png",
      "icon" : "images/distributed-time-icon.png"
    },
    "dependencies": [
      {
        "masterElectionStrategy": "FIRST_NODE",
        "masterService": "distributed-time",
        "numberOfMasters": 1,
        "mandatory": true,
        "restart": true
      }
    ],
    "commands" : [
      {
        "id" : "show_log",
        "name" : "Show Logs",
        "command": "cat /var/log/distributed-time/distributed-time.log",
        "icon": "fa-file"
      }
    ]
  },



  "cluster-manager": {
    "config": {
      "imageName" : "cluster-manager",
      "order": 1,
      "unique": true,
      "name" : "Cluster Manager",
      "selectionLayout" : { "row" : 1, "col" : 2},
      "memory": "small",
      "logo" : "images/cluster-manager-logo.png",
      "icon" : "images/cluster-manager-icon.png"
    },
    "dependencies": [
      {
        "masterElectionStrategy": "RANDOM",
        "masterService": "cluster-manager",
        "numberOfMasters": 1,
        "mandatory": true,
        "restart": true
      }
    ],
    "additionalEnvironment": [ "SERVICE_NUMBER_1_BASED" ],
    "editableSettings": [
      {
        "filename": "environment",
        "propertyType": "REGEX",
        "propertyFormat": "{name}{value}",
        "commentPrefix": "#",
        "filesystemService": "cluster-manager",
        "properties": [
          {
            "name": "Xms",
            "comment": "Startup Heap Size for Cluster manager\n [ESKIMO_DEFAULT] means memory allocator will decide of Cluster Manager memory share.",
            "validationRegex": "^([0-9\\.]+[kmgt]?)$|^(\\[ESKIMO_DEFAULT\\])$",
            "defaultValue": "[ESKIMO_DEFAULT]"
          },
          {
            "name": "Xmx",
            "comment": "Maximum Heap Size for Cluster Manager\n [ESKIMO_DEFAULT] means memory allocator will decide of Cluster Manager memory share.",
            "validationRegex": "^([0-9\\.]+[kmgt]?)$|^(\\[ESKIMO_DEFAULT\\])$",
            "defaultValue": "[ESKIMO_DEFAULT]"
          }
        ]
      }
    ],
    "commands" : [
      {
        "id" : "show_log",
        "name" : "Show Logs",
        "command": "cat /var/log/cluster-manager/cluster-manager.log",
        "icon": "fa-file"
      }
    ]
  },



  "distributed-filesystem" : {
    "config": {
      "imageName" : "distributed-filesystem",
      "order": 2,
      "mandatory": true,
      "conditional" : "MULTIPLE_NODES",
      "name" : "Distributed Filesystem",
      "selectionLayout" : { "row" : 2, "col" : 1},
      "memory": "negligible",
      "logo" : "images/distributed-filesystem-logo.png",
      "icon" : "images/distributed-filesystem-icon.png"
    },
    "ui": {
      "urlTemplate": "./distributed-filesystem/{NODE_ADDRESS}/management/app.html",
      "proxyTargetPort": 28901,
      "waitTime": 10000,
      "role" : "ADMIN",
      "title": "Distributed FileSystem Dashboard"
    },
    "masterDetection": {
      "strategy" : "LOG_FILE",
      "logFile" : "/var/log/distributed-filesystem/management/management.log",
      "grep": "I am the new leader",
      "timeStampExtractRexp" : "([0-9\\-]+ [0-9.:,]+).*",
      "timeStampFormat" : "yyyy-MM-dd HH:mm:ss,SSS"
    },
    "dependencies": [
      {
        "masterElectionStrategy": "FIRST_NODE",
        "masterService": "cluster-manager",
        "numberOfMasters": 1,
        "mandatory": true,
        "restart": true
      }
    ],
    "additionalEnvironment": [
      "ALL_NODES_LIST_distributed-filesystem"
    ],
    "editableSettings": [
      {
        "filename": "management.properties",
        "filesystemService": "management",
        "propertyType": "variable",
        "propertyFormat": "{name}={value}",
        "commentPrefix": "#",
        "properties": [
          {
            "name": "target.volumes",
            "comment": "The volumes to be automagically managed by distributed-filesystem Management.",
            "defaultValue": "",
            "value": "calculator_eventlog,calculator_data,broker_data,cluster_shared"
          },
          {
            "name": "target.volumes.performance.off",
            "comment": "Volumes for which the performance setting sneeds to be turned off\n.",
            "defaultValue": "",
            "value": "broker_data"
          },
          {
            "name": "config.performance.off",
            "comment": "Performance settings to turn off for volumes defined in 'target.volumes.performance.off'",
            "defaultValue": "",
            "value": "performance.quick-read,performance.io-cache,performance.write-behind,performance.stat-prefetch,performance.read-ahead,performance.readdir-ahead,performance.open-behind"
          },
          {
            "name": "system.statusUpdatePeriodSeconds",
            "comment": "The orchestration loop delay in seconds. distributed-filesystem Management runs its checks and updates its status every X seconds.",
            "validationRegex": "^[0-9\\.]+$",
            "defaultValue": "30",
            "value": "20"
          }
        ]
      }
    ],
    "commands" : [
      {
        "id" : "show_log",
        "name" : "Show distributed-filesystem Logs",
        "command": "sudo cat /var/log/distributed-filesystem/distributed-filesystem.log",
        "icon": "fa-file"
      },
      {
        "id" : "show_remote_log",
        "name" : "Show Remote Server Logs",
        "command": "cat /var/log/distributed-filesystem/distributed-filesystem_remote-log",
        "icon": "fa-file"
      }
    ]
  },


  "cluster-master": {
    "config": {
      "imageName" : "cluster-master",
      "order": 3,
      "kubeMaster" : true,
      "unique": true,
      "group" : "Cluster",
      "name" : "Master",
      "selectionLayout" : { "row" : 2, "col" : 2},
      "memory": "negligible",
      "logo" : "images/cluster-master-logo.png",
      "icon" : "images/cluster-master-icon.png",
      "user": {
        "name": "cluster",
        "id": 1001
      }
    },
    "dependencies": [
      {
        "masterElectionStrategy": "SAME_NODE",
        "masterService": "distributed-filesystem",
        "numberOfMasters": 1,
        "mandatory": false,
        "restart": true
      },
      {
        "masterElectionStrategy": "FIRST_NODE",
        "masterService": "cluster-master",
        "numberOfMasters": 1,
        "mandatory": true,
        "restart": true
      },
      {
        "masterElectionStrategy": "SAME_NODE",
        "masterService": "cluster-slave",
        "numberOfMasters": 1,
        "mandatory": true,
        "restart": false,
        "dependentInstalledFirst": true
      }
    ]
  },



  "cluster-slave": {
    "config": {
      "order": 4,
      "kubeSlave" : true,
      "group" : "Cluster",
      "name" : "Slave",
      "selectionLayout" : { "row" : 5, "col" : 1},
      "memory": "negligible",
      "logo" : "images/cluster-slave-logo.png",
      "icon" : "images/cluster-slave-icon.png",
      "user": {
        "name": "cluster",
        "id": 1001
      }
    },
    "dependencies": [
      {
        "masterElectionStrategy": "FIRST_NODE",
        "masterService": "cluster-master",
        "numberOfMasters": 1,
        "mandatory": true,
        "restart": true,
        "hooks": {
          "preUninstallHook": " /usr/local/bin/kubectl drain {service.node.address} --force --timeout=300s"
        }
      },
      {
        "masterElectionStrategy": "SAME_NODE",
        "masterService": "distributed-filesystem",
        "numberOfMasters": 1,
        "mandatory": true,
        "restart": true
      }
    ],
    "additionalEnvironment": [
      "ALL_NODES_LIST_cluster-slave",
      "SERVICE_NUMBER_1_BASED"
    ]
  },



  "cluster-dashboard" : {
    "config": {
      "imageName" : "cluster-dashboard",
      "order": 5,
      "kubernetes": true,
      "kubeConfig": {
        "request": {
          "cpu": "0.3",
          "ram": "500M"
        }
      },
      "unique": true,
      "group" : "Cluster",
      "name" : "Dashboard",
      "memory": "small",
      "logo" : "images/cluster-master-logo.png",
      "icon" : "images/cluster-master-icon.png",
      "user": {
        "name": "cluster",
        "id": 1001
      }
    },
    "webCommands": [
      {
        "id" : "clusterDashboardLoginToken",
        "service": "cluster-master",
        "command": "/usr/local/bin/kubectl get secret `/usr/local/bin/kubectl get sa/eskimo -o jsonpath=\"{.secrets[0].name}\"` -o go-template=\"{{.data.token | base64decode}}\""
      }
    ],
    "ui": {
      "kubeProxy": true,
      "__comment_urlTemplate": "Going through Kubectl proxy.",
      "urlTemplate": "cluster-dashboard/api/v1/namespaces/cluster-dashboard/services/https:cluster-dashboard:/proxy/#/workloads?namespace=_all",
      "__comment_proxyTargetPort": "Kubectl proxy target port",
      "proxyTargetPort" : 8001,
      "waitTime": 5000,
      "role" : "ADMIN",
      "title" : "Cluster Dashboard",
      "proxyReplacements" : [
        {
          "type" : "PLAIN",
          "source" : "h=\"https\"===h.slice(0,5)?\"wss\"+h.slice(5):\"ws\"+h.slice(4),this.url=h,",
          "target" : "h=\"https\"===h.slice(0,5)?\"wss\"+h.slice(5):\"ws\"+h.slice(4),this.url=h.replace(\"kubernetes-dashboard/api\",\"ws/kubernetes-dashboard/api\"),"
        },
        {
          "type" : "PLAIN",
          "source" : "\"WebSocket connection broken\"),d._cleanup()}}",
          "target" : "\"WebSocket connection broken\"),d._cleanup()};let eskThat=this;eskThat.ws.onopen = function (event) {eskThat.ws.send(\"HELLO_ESKIMO\");};}"
        },
        {
          "type" : "PLAIN",
          "source" : "kubernetes-dashboard//",
          "target" : "kubernetes-dashboard/"
        },
        {
          "__comment": "ALl of the following are to get rid of the limit of Kubernetes Dashboard to prevent HTTP access from non-localhost",
          "type" : "PLAIN",
          "source" : "isCurrentDomainSecure_(){return[\"localhost\",\"127.0.0.1\"].indexOf(location.hostname)>-1}",
          "target" : "isCurrentDomainSecure_(){return true}"
        },
        {
          "type" : "PLAIN",
          "source" : "isCurrentProtocolSecure_(){return location.protocol.includes(\"https\")}",
          "target" : "isCurrentProtocolSecure_(){return true}"
        },
        {
          "type" : "PLAIN",
          "source" : "isAuthenticationEnabled(S){return S.httpsMode}",
          "target" : "isAuthenticationEnabled(S){return true}"
        },
        {
          "type" : "PLAIN",
          "source" : "isAuthEnabled(){return!!this.loginStatus&&this.loginStatus.httpsMode}",
          "target" : "isAuthEnabled(){return true}"
        },
        {
          "type" : "PLAIN",
          "source" : "isLoginPageEnabled(){return\"true\"!==this.cookies_.get(this.config_.skipLoginPageCookieName)}",
          "target" : "isLoginPageEnabled(){return true}"
        },
        {
          "type" : "PLAIN",
          "source" : "isLoginEnabled(){return this.isCurrentDomainSecure_()||this.isCurrentProtocolSecure_()}",
          "target" : "isLoginEnabled(){return true}"
        },
        {
          "type" : "PLAIN",
          "source" : ",S.secure&&(v+=\"secure;\"),S.sameSite||(S.sameSite=\"Lax\"),v+=\"sameSite=\"+S.sameSite+\";\"",
          "target" : ""
        }

      ],
      "pageScripters" : [
        {
          "resourceUrl" : "api/v1/namespaces/cluster-dashboard/services/https:cluster-dashboard:/proxy/",
          "script": "function eskimoLoginChecker() {\n        \n    let contextPath = \"{CONTEXT_PATH}\";\n        \n    if (document.getElementsByTagName (\"kd-login\").length >= 1) {\n\n        document.getElementsByClassName(\"mat-radio-input\")[0].click();\n        \n        const fetchToken = async () => {\n            const response = await fetch((contextPath != \"\" ? \"/\" + contextPath : \"/\") + \"eskimo-command/clusterDashboardLoginToken\");\n            const result = await response.json(); //extract JSON from the http response\n\n            const error = result.error;\n            if (error && error != \"\") {\n                console.log (error);\n            } else {\n\n                const loginToken = result.value;\n\n                const tokenInput = document.getElementById(\"token\");\n                tokenInput.value = loginToken;\n                \n                if (\"createEvent\" in document) {\n                    var evt = document.createEvent(\"HTMLEvents\");\n                    evt.initEvent(\"change\", false, true);\n                    tokenInput.dispatchEvent(evt);\n                } else {\n                    tokenInput.fireEvent(\"onchange\");\n                }                \n        \n                setTimeout (function() { \n                    tokenInput.value = '';\n                    tokenInput.setAttribute('readonly','readonly');                \n                    document.getElementsByTagName(\"button\")[0].click(); \n                }, 50);\n            }\n        }    \n        \n        fetchToken();\n    }\n    \n    setTimeout (function() { eskimoLoginChecker();}, 3000);\n}\n\neskimoLoginChecker();"
        }
      ]
    }
  },



  "broker" : {
    "config": {
      "imageName" : "broker",
      "order": 6,
      "kubernetes": true,
      "kubeConfig": {
        "request": {
          "cpu": "1",
          "ram": "1G"
        }
      },
      "group" : "Broker",
      "name" : "runtime",
      "memory": "medium",
      "logo" : "images/broker-logo.png",
      "icon" : "images/broker-icon.png",
      "user": {
        "name": "broker",
        "id": 1002
      }
    },
    "dependencies": [
      {
        "masterElectionStrategy": "FIRST_NODE",
        "masterService": "cluster-manager",
        "numberOfMasters": 1,
        "mandatory": true,
        "restart": true
      },
      {
        "masterElectionStrategy": "ALL_NODES",
        "masterService": "cluster-slave",
        "numberOfMasters": 1,
        "mandatory": true,
        "restart": true
      }
    ],
    "editableSettings": [
      {
        "filename": "eskimo-memory.opts",
        "propertyType": "REGEX",
        "propertyFormat": "{name}{value}",
        "commentPrefix": "#",
        "filesystemService": "broker",
        "properties": [
          {
            "name": "Xms",
            "comment": "Startup Heap Size for Broker java process\n [ESKIMO_DEFAULT] means memory allocator will decide of Broker memory share.",
            "validationRegex": "^([0-9\\.]+[kmgt]?)$|^(\\[ESKIMO_DEFAULT\\])$",
            "defaultValue": "[ESKIMO_DEFAULT]"
          },
          {
            "name": "Xmx",
            "comment": "Maximum Heap Size for Broker java process\n [ESKIMO_DEFAULT] means memory allocator will decide of Broker memory share.",
            "validationRegex": "^([0-9\\.]+[kmgt]?)$|^(\\[ESKIMO_DEFAULT\\])$",
            "defaultValue": "[ESKIMO_DEFAULT]"
          }
        ]
      },
      {
        "filename": "server.properties",
        "filesystemService": "broker",
        "propertyType": "variable",
        "propertyFormat": "{name}={value}",
        "commentPrefix": "#",
        "properties": [
          {
            "name": "num.network.threads",
            "comment": "The number of threads that the server uses for receiving requests from the network and sending responses to the network",
            "validationRegex": "^[0-9\\.]+$",
            "defaultValue": "3"
          },
          {
            "name": "num.io.threads",
            "comment": "The number of threads that the server uses for processing requests, which may include disk I/O",
            "validationRegex": "^[0-9\\.]+$",
            "defaultValue": "8"
          },
          {
            "name": "socket.send.buffer.bytes",
            "comment": "The send buffer (SO_SNDBUF) used by the socket server",
            "validationRegex": "^[0-9\\.]+$",
            "defaultValue": "102400"
          },
          {
            "name": "socket.receive.buffer.bytes",
            "comment": "The receive buffer (SO_RCVBUF) used by the socket server",
            "validationRegex": "^[0-9\\.]+$",
            "defaultValue": "102400"
          },
          {
            "name": "socket.request.max.bytes",
            "comment": "The maximum size of a request that the socket server will accept (protection against OOM)",
            "validationRegex": "^[0-9\\.]+$",
            "defaultValue": "104857600"
          },
          {
            "name": "num.partitions",
            "comment": "The default number of log partitions per topic. More partitions allow greater parallelism for consumption, but this will also result in more files across the brokers.",
            "validationRegex": "^[0-9\\.]+$",
            "defaultValue": "1"
          },
          {
            "name": "log.retention.hours",
            "comment": "The minimum age of a log file to be eligible for deletion due to age",
            "validationRegex": "^[0-9\\.]+$",
            "defaultValue": "168"
          }
        ]
      }
    ],
    "commands" : [
      {
        "id" : "show_server_log",
        "name" : "Show Server Logs",
        "command": "cat /var/log/broker/server.log",
        "icon": "fa-file"
      },
      {
        "id" : "show_controller_log",
        "name" : "Show Controller Logs",
        "command": "cat /var/log/broker/controller.log",
        "icon": "fa-file"
      },
      {
        "id" : "show_state_change_log",
        "name" : "Show State Change Logs",
        "command": "cat /var/log/broker/state-change.log",
        "icon": "fa-file"
      },
      {
        "id" : "mount_broker_data",
        "name" : "Mount broker_data on Host",
        "command": "sudo /usr/local/sbin/gluster-mount.sh broker_data /var/lib/kafka/data broker",
        "icon": "fa-folder"
      }
    ]
  },



  "broker-cli" : {
    "config": {
      "order": 7,
      "group": "Broker",
      "name": "Client",
      "selectionLayout": {
        "row": 2,
        "col": 3
      },
      "memory": "negligible",
      "logo" : "images/broker-logo.png",
      "icon" : "images/broker-icon.png",
      "user": {
        "name": "broker",
        "id": 1002
      }
    },
    "commands" : [
      {
        "id" : "mount_broker_data_brokercli",
        "name" : "Mount broker_data on Host",
        "command": "sudo /usr/local/sbin/gluster-mount.sh broker_data /var/lib/broker/data broker",
        "icon": "fa-folder"
      }
    ]
  },



  "broker-manager": {
    "config": {
      "imageName" : "broker-manager",
      "order": 8,
      "kubernetes": true,
      "kubeConfig": {
        "request": {
          "cpu": "0.3",
          "ram": "400M"
        }
      },
      "unique": true,
      "group" : "Broker",
      "name" : "Manager",
      "memory": "small",
      "logo" : "images/broker-manager-logo.png",
      "icon" : "images/broker-manager-icon.png",
      "user": {
        "name": "broker",
        "id": 1002
      }
    },
    "ui": {
      "kubeProxy": true,
      "__comment_urlTemplate": "Going through Kubectl proxy.",
      "urlTemplate": "broker-manager/api/v1/namespaces/eskimo/services/broker-manager:31220/proxy/",
      "__comment_proxyTargetPort": "Kubectl proxy target port",
      "proxyTargetPort" : 8001,
      "waitTime": 6000,
      "role" : "ADMIN",
      "title" : "Broker manager",
      "proxyReplacements" : [
        {
          "type" : "PLAIN",
          "source" : "{PREFIX_PATH}/{PREFIX_PATH}",
          "target" : "{PREFIX_PATH}"
        }
      ]
    },
    "dependencies": [
      {
        "masterElectionStrategy": "FIRST_NODE",
        "masterService": "cluster-manager",
        "numberOfMasters": 1,
        "mandatory": true,
        "restart": true
      },
      {
        "masterElectionStrategy": "RANDOM",
        "masterService": "broker",
        "numberOfMasters": 1,
        "mandatory": true,
        "restart": false
      }
    ],
    "commands" : [
      {
        "id" : "show_log",
        "name" : "Show Logs",
        "command": "cat /var/log/kafka/broker-manager/application.log",
        "icon": "fa-file"
      },
      {
        "id" : "mount_broker_data_brokermgr",
        "name" : "Mount broker_data on Host",
        "command": "sudo /usr/local/sbin/gluster-mount.sh broker_data /var/lib/broker/data broker",
        "icon": "fa-folder"
      }
    ]
  },



  "calculator-runtime" : {
    "config": {
      "imageName" : "calculator",
      "order": 9,
      "group" : "Calculator",
      "name" : "Runtime",
      "kubernetes": true,
      "kubeConfig": {
        "request": {
          "cpu": "1",
          "ram": "800M"
        }
      },
      "registryOnly": true,
      "memory": "large",
      "logo" : "images/calculator-runtime-logo.png",
      "icon" : "images/calculator-runtime-icon.png",
      "user": {
        "name": "calculator",
        "id": 1003
      }
    },
    "dependencies": [
      {
        "masterElectionStrategy": "FIRST_NODE",
        "masterService": "cluster-master",
        "numberOfMasters": 1,
        "mandatory": true,
        "restart": false
      }
    ],
    "editableSettings": [
      {
        "filename": "calculator-defaults.conf",
        "propertyType": "variable",
        "propertyFormat": "{name}={value}",
        "commentPrefix": "#",
        "filesystemService": "spark",
        "properties" : [
          {
            "name" :  "calculator.driver.memory",
            "comment": "Limiting the driver (client) memory",
            "validationRegex": "^([0-9\\.]+[kmgt]?)$|^(\\[ESKIMO_DEFAULT\\])$",
            "defaultValue" : "800m"
          },
          {
            "name" :  "calculator.rpc.numRetries",
            "comment": "Number of times to retry before an RPC task gives up. An RPC task will run at most times of this number.",
            "validationRegex": "^[0-9\\.]+$",
            "defaultValue" : "5"
          },
          {
            "name" :  "calculator.rpc.retry.wait",
            "comment": "Duration for an RPC ask operation to wait before retrying.",
            "validationRegex": "^[0-9\\.]+s$",
            "defaultValue" : "5s"
          },
          {
            "name" :  "calculator.scheduler.mode",
            "comment": "The scheduling mode between jobs submitted to the same CalculatorContext. \nCan be FIFO or FAIR. FAIR Seem not to work well with Kubernetes",
            "validationRegex": "^(FAIR)$|^(FIFO))$",
            "defaultValue" : "FAIR"
          },
          {
            "name" :  "calculator.locality.wait",
            "comment": "How long to wait to launch a data-local task before giving up and launching it on a less-local node.",
            "validationRegex": "^[0-9\\.]+s$",
            "defaultValue" : "20s"
          },
          {
            "name" :  "calculator.dynamicAllocation.executorIdleTimeout",
            "comment": "If dynamic allocation is enabled and an executor has been idle for more than this duration, the executor will be removed. \n (Caution here : small values cause issues. I have executors killed with 10s for instance)",
            "validationRegex": "^[0-9\\.]+s$",
            "defaultValue" : "200s"
          },
          {
            "name" :  "calculator.dynamicAllocation.cachedExecutorIdleTimeout",
            "comment": "If dynamic allocation is enabled and an executor which has cached data blocks has been idle for more than this duration, the executor will be removed - should be consistent with spark.dynamicAllocation.shuffleTracking.timeout. \n (Caution here : small values cause issues. I have executors killed with 10s for instance)",
            "validationRegex": "^[0-9\\.]+s$",
            "defaultValue" : "300s"
          },
          {
            "name" :  "calculator.dynamicAllocation.shuffleTracking.timeout",
            "comment": "When shuffle tracking is enabled, controls the timeout for executors that are holding shuffle data - should be consistent with spark.dynamicAllocation.cachedExecutorIdleTimeout.",
            "validationRegex": "^[0-9\\.]+s$",
            "defaultValue" : "300s"
          },
          {
            "name" :  "calculator.dynamicAllocation.schedulerBacklogTimeout",
            "comment": "\tIf dynamic allocation is enabled and there have been pending tasks backlogged for more than this duration, new executors will be requested.",
            "validationRegex": "^[0-9\\.]+s$",
            "defaultValue" : "5s"
          },
          {
            "name" :  "calculator.executor.memory",
            "comment": "Defining default Spark executor memory allowed by Eskimo Memory Management (found in topology). \nUSE [ESKIMO_DEFAULT] to leave untouched or e.g. 800m, 1.2g, etc.",
            "validationRegex": "^([0-9\\.]+[kmgt]?)$|^(\\[ESKIMO_DEFAULT\\])$",
            "defaultValue" : "[ESKIMO_DEFAULT]"
          }
        ]
      }
    ],
    "commands" : [
      {
        "id" : "mount_calculator_eventlog",
        "name" : "Mount calculator_eventlog on Host",
        "command": "sudo /usr/local/sbin/gluster-mount.sh calculator_eventlog /var/lib/calculator/eventlog calculator",
        "icon": "fa-folder"
      },
      {
        "id" : "mount_calculator_data",
        "name" : "Mount calculator_data on Host",
        "command": "sudo /usr/local/sbin/gluster-mount.sh calculator_data /var/lib/calculator/data calculator",
        "icon": "fa-folder"
      }
    ]
  },



  "calculator-cli" : {
    "config": {
      "order": 10,
      "group" : "Calculator",
      "name": "Client",
      "selectionLayout": {
        "row": 3,
        "col": 3
      },
      "memory": "negligible",
      "logo" : "images/calculator-runtime-logo.png",
      "icon" : "images/calculator-runtime-icon.png",
      "user": {
        "name": "calculator",
        "id": 1003
      }
    },
    "commands" : [
      {
        "id" : "mount_calculator_eventlog_calculatorcli",
        "name" : "Mount calculator_eventlog on Host",
        "command": "sudo /usr/local/sbin/gluster-mount.sh spark_eventlog /var/lib/calculator/eventlog calculator",
        "icon": "fa-folder"
      },
      {
        "id" : "mount_calculator_data_calculatorcli",
        "name" : "Mount calculator_data on Host",
        "command": "sudo /usr/local/sbin/gluster-mount.sh calculator_data /var/lib/calculator/data calculator",
        "icon": "fa-folder"
      }
    ]
  },



  "database-manager" : {
    "config": {
      "imageName" : "database-manager",
      "order": 11,
      "kubernetes": true,
      "kubeConfig": {
        "request": {
          "cpu": "0.3",
          "ram": "400M"
        }
      },
      "unique": true,
      "group" : "Database",
      "name" : "manager",
      "memory": "small",
      "logo" : "images/database-manager-logo.png",
      "icon" : "images/database-manager-icon.png",
      "user": {
        "name": "database",
        "id": 1004
      }
    },
    "ui": {
      "kubeProxy": true,
      "__comment_urlTemplate": "Going through Kubectl proxy.",
      "urlTemplate": "database-manager/api/v1/namespaces/eskimo/services/cerebro:31900/proxy/#/overview?host=http:%2F%2Fdatabase.default.svc.cluster.eskimo:9200",
      "__comment_proxyTargetPort": "Kubectl proxy target port",
      "proxyTargetPort" : 8001,
      "waitTime": 5000,
      "role" : "*",
      "title" : "Database Manager"
    },
    "dependencies": [
      {
        "masterElectionStrategy": "RANDOM",
        "masterService": "database",
        "numberOfMasters": 1,
        "mandatory": true,
        "restart": false
      }
    ],
    "editableSettings": [
      {
        "filename": "JVM_OPTS.sh",
        "propertyType": "REGEX",
        "propertyFormat": "{name}{value}",
        "commentPrefix": "#",
        "filesystemService": "database-manager",
        "properties": [
          {
            "name": "Xms",
            "comment": "Startup Heap Size for database-manager java process\n[ESKIMO_DEFAULT] means memory allocator will decide of Cerebro memory share.",
            "validationRegex": "^([0-9\\.]+[kmgt]?)$|^(\\[ESKIMO_DEFAULT\\])$",
            "defaultValue": "[ESKIMO_DEFAULT]"
          },
          {
            "name": "Xmx",
            "comment": "Maximum Heap Size for database-manager java process\n[ESKIMO_DEFAULT] means memory allocator will decide of Cerebro memory share.",
            "validationRegex": "^([0-9\\.]+[kmgt]?)$|^(\\[ESKIMO_DEFAULT\\])$",
            "defaultValue": "[ESKIMO_DEFAULT]"
          }
        ]
      }
    ]
  },



  "database": {
    "config": {
      "imageName" : "database",
      "order": 12,
      "kubernetes": true,
      "kubeConfig": {
        "request": {
          "cpu": "1",
          "ram": "1G"
        }
      },
      "group" : "Database",
      "name" : "Runtime",
      "memory": "large",
      "logo" : "images/database-logo.png",
      "icon" : "images/database-icon.png",
      "user": {
        "name": "database",
        "id": 1004
      }
    },
    "dependencies": [
      {
        "masterElectionStrategy": "ALL_NODES",
        "masterService": "cluster-slave",
        "numberOfMasters": 1,
        "mandatory": true,
        "restart": true
      }
    ],
    "editableSettings": [
      {
        "filename": "eskimo.options",
        "propertyType": "REGEX",
        "propertyFormat": "{name}{value}",
        "commentPrefix": "#",
        "filesystemService": "database",
        "properties": [
          {
            "name": "Xms",
            "comment": "Startup Heap Size for ES java process\n[ESKIMO_DEFAULT] means memory allocator will decide of ES memory share.\n<strong>THIS NEEDS TO BE THE SAME VALUE AS Xmx OTHERWISE ES WILL LIKELY REFUSE TO START</strong>.",
            "validationRegex": "^([0-9\\.]+[kmgt]?)$|^(\\[ESKIMO_DEFAULT\\])$",
            "defaultValue": "[ESKIMO_DEFAULT]"
          },
          {
            "name": "Xmx",
            "comment": "Maximum Heap Size for ES java process\n[ESKIMO_DEFAULT] means memory allocator will decide of ES memory share.\n<strong>THIS NEEDS TO BE THE SAME VALUE AS Xms OTHERWISE ES WILL LIKELY REFUSE TO START</strong>.",
            "validationRegex": "^([0-9\\.]+[kmgt]?)$|^(\\[ESKIMO_DEFAULT\\])$",
            "defaultValue": "[ESKIMO_DEFAULT]"
          }
        ]
      },
      {
        "filename": "database.yml",
        "propertyType": "variable",
        "propertyFormat": "{name}: {value}",
        "commentPrefix": "#",
        "filesystemService": "database",
        "properties": [
          {
            "name": "action.destructive_requires_name",
            "comment": "Require explicit names when deleting indices",
            "validationRegex": "^(true)$|^(false)$",
            "defaultValue": "false"
          }
        ]
      },
      {
        "filename": "database-index-defaults.properties",
        "propertyType": "variable",
        "propertyFormat": "{name}={value}",
        "commentPrefix": "#",
        "filesystemService": "database",
        "properties" : [
          {
            "name" :  "index.refresh_interval",
            "comment": "Default refresh interval on new indices. Use format such as 10s, 1m, etc. \nUSE [ESKIMO_DEFAULT] to leave ElasticSearch default value.",
            "validationRegex": "^([0-9\\.]+[smh]?)$|^(\\[ESKIMO_DEFAULT\\])$",
            "defaultValue" : "[ESKIMO_DEFAULT]"
          },
          {
            "name" :  "index.number_of_replicas",
            "comment": "Default number of additional replicas on new indices.  \nUSE [ESKIMO_DEFAULT] to let Eskimo compute best value.",
            "validationRegex": "^([0-9\\.]+)$|^(\\[ESKIMO_DEFAULT\\])$",
            "defaultValue" : "[ESKIMO_DEFAULT]"
          },
          {
            "name" :  "index.number_of_shards",
            "comment": "Default number of additional replicas on new indices.  \nUSE [ESKIMO_DEFAULT] to leave ElasticSearch default value (5).",
            "validationRegex": "^([0-9\\.]+)$|^(\\[ESKIMO_DEFAULT\\])$",
            "defaultValue" : "[ESKIMO_DEFAULT]"
          }
        ]
      }
    ],
    "commands" : [
      {
        "id" : "show_log",
        "name" : "Show Logs",
        "command": "cat /var/log/database/eskimo.log",
        "icon": "fa-file"
      },
      {
        "id" : "show_search_slow_log",
        "name" : "Show Search Slow Logs",
        "command": "cat /var/log/database/eskimo_index_search_slowlog.log",
        "icon": "fa-file"
      },
      {
        "id" : "show_index_slow_log",
        "name" : "Show Indexing Slow Logs",
        "command": "cat /var/log/database/eskimo_index_indexing_slowlog.log",
        "icon": "fa-file"
      },
      {
        "id" : "show_gc_log",
        "name" : "Show GC Logs",
        "command": "cat /var/log/database/gc.log",
        "icon": "fa-file"
      }
    ]
  },



  "user-console" : {
    "config": {
      "imageName" : "user-console",
      "order": 13,
      "unique": true,
      "kubernetes": true,
      "kubeConfig": {
        "request": {
          "cpu": "1",
          "ram": "1G"
        }
      },
      "name" : "User Console",
      "memory": "verylarge",
      "____comment": "We need to know about the calculator memory to set",
      "memoryAdditional__commentedOut": ["calculator-runtime"],
      "logo" : "images/zeppelin-logo.png",
      "icon" : "images/zeppelin-icon.png",
      "user": {
        "name": "calculator",
        "id": 1003
      }
    },
    "ui": {
      "kubeProxy": true,
      "__comment_urlTemplate": "Going through Kubectl proxy.",
      "urlTemplate": "zeppelin/api/v1/namespaces/eskimo/services/zeppelin:31008/proxy/",
      "__comment_proxyTargetPort": "Kubectl proxy target port",
      "proxyTargetPort" : 8001,
      "waitTime": 8000,
      "role" : "*",
      "title" : "USer Console",
      "____comment": "the following is recovered from spark console to have a case to test here",
      "urlRewriting" : [
        {
          "startUrl" : "{FULL_SERVER_ROOT}/history/",
          "replacement" : "{FULL_SERVER_ROOT}/user-console/history/"
        },
        {
          "startUrl" : "{FULL_SERVER_ROOT_NO_CONTEXT}/history/",
          "replacement" : "{FULL_SERVER_ROOT}/user-console/history/"
        }
      ],
      "proxyReplacements" : [
        {
          "type" : "PLAIN",
          "source" : "return t+\"//\"+location.hostname+\":\"+this.getPort()+e(location.pathname)+\"/ws",
          "target" : "return t + \"//\" + location.hostname + \":\" + this.getPort() + \"/{CONTEXT_PATH}ws\" + e(location.pathname).replace(\"{CONTEXT_PATH}\" != \"\" ? \"{CONTEXT_PATH}\" : \"dummy_not_matching_anything\", \"\") + \"/ws"
        },
        {
          "type" : "PLAIN",
          "source" : "!function(e){var t={};",
          "target" : "function noOp(){}; !function(e){var t={};"
        },
        {
          "type" : "PLAIN",
          "source" : "console.log(\"Send",
          "target" : "noOp(\"Send"
        },
        {
          "type" : "PLAIN",
          "source" : "console.log(\"Receive",
          "target" : "noOp(\"Receive"
        },
        {
          "type" : "PLAIN",
          "source" : "<li><a href=\"/zeppelin/next\">Try the new Zeppelin</a></li>",
          "target" : ""
        }
      ]
    },
    "dependencies": [
      {
        "masterElectionStrategy": "RANDOM",
        "masterService": "database",
        "numberOfMasters": 1,
        "mandatory": false,
        "restart": false
      },
      {
        "masterElectionStrategy": "FIRST_NODE",
        "masterService": "cluster-manager",
        "numberOfMasters": 1,
        "mandatory": true,
        "restart": true
      },
      {
        "masterElectionStrategy": "RANDOM",
        "masterService": "distributed-filesystem",
        "numberOfMasters": 1,
        "mandatory": false,
        "restart": true
      },
      {
        "masterElectionStrategy": "RANDOM",
        "masterService": "broker",
        "numberOfMasters": 1,
        "mandatory": false,
        "restart": false
      }
    ],
    "editableSettings": [
      {
        "filename": "user-console-env.sh",
        "filesystemService": "user-console",
        "propertyType": "REGEX",
        "propertyFormat": "{name}{value}",
        "commentPrefix": "#",
        "properties": [
          {
            "name": "Xmx",
            "comment": "Maximum heap size memory allocated to user-console process.\n[ESKIMO_DEFAULT] leaves the default memory allocation strategy decide of it.",
            "validationRegex": "^([0-9\\.]+[kmgt]?)$|^(\\[ESKIMO_DEFAULT\\])$",
            "defaultValue": "[ESKIMO_DEFAULT]"
          }
        ]
      },
      {
        "filename": "eskimo_settings.conf",
        "filesystemService": "user-console",
        "propertyType": "variable",
        "propertyFormat": "{name}={value}",
        "commentPrefix": "#",
        "properties": [
          {
            "name": "zeppelin_note_isolation",
            "comment": "The setting 'zeppelin_note_isolation' is used to control whether interpreter processes are created and managed globally for the whole zeppelin process or per note.\nPossible values are:\n 'shared' : one single instance of every interpreter is created and shared among users and notes (better for laboratory).\n 'per_note' : one instance of interpreter is created for every note (better for production - but requires a lot of RAM).\n",
            "validationRegex": "^(per_note)$|^(shared)$",
            "defaultValue": "shared"
          }
        ]
      }
    ],
    "commands" : [
      {
        "id" : "show_log",
        "name" : "Show Logs",
        "command": "sudo cat /var/log/calculator/user-console/`ls -t /var/log/calculator/user-console/ | grep user-console-eskimo- | head -n 1`",
        "icon": "fa-file"
      }
    ]
  }

}